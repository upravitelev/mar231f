[["index.html", "Методы анализа больших данных в исследованиях поведения покупателей Intro", " Методы анализа больших данных в исследованиях поведения покупателей Ph.A.Upravitelev 2024-12-26 Intro September 9: О курсе + задачи продуктовых аналитиков. Знакомство. Структура курса. Организационная информация по курсу. Виды аналитиков. Продукт, продуктовая аналитика. Роли продуктовых аналитиков в командах. Грейды. Этапы развития продукта и задачи аналитиков на каждом этапе. Бизнес-модели. "],["c1_intro.html", "О курсе и аналитиках Запись занятия Обо мне Contacts О курсе Виды аналитиков Продуктовая аналитика Стадии развития продукта Бизнес-модели Полезные материалы Домашнее задание", " О курсе и аналитиках Запись занятия Обо мне продуктовый аналитик в Pixonic продуктовый аналитик в GameInsight аналитик в Консультант+ аспирант СПбГУ (когнитивная психология) Contacts @konhis в telegram upravitelev@gmail.com (дополнительное средство коммуникации) О курсе основные темы Введение в цели и задачи продуктовой аналитики Метрики активности и вовлечения пользователей Основы юнит-экономики и метрики монетизации пользователей Проверка гипотез А/B-тесты дополнительные темы SQL разметка событий дашборды приглашенные лекторы (UX, аналитика) разбор кейсов формы контроля две контрольные работы две домашние работы накопительная оценка по формуле 0.1 * Кр1 + 0.35 * Др1 + 0.2 * Кр2 + 0.35 * Др2, округление арифметическое. Виды аналитиков data scientists Датасаентисты - общее определение нескольких профессий. Основной набор навыков – математика, программирование и знание бизнес-задач. Сочетание этих навыков в разных пропорциях и характеризует разные виды датасаентистов. Аналитики должны хорошо понимать бизнес-задачи проекта и специфику бизнеса, к тому же сейчас профессия аналитика предполагает хорошее знание статистики и хотя бы начальные навыки программирования. web-аналитика Задачи: сбор и анализ данных о посетителях веб-сайтов и их поведении на сайте Инструменты: Google Analytics, Яндекс.Метрика, Google Tag Manager маркетинговая аналитика Задачи: оценка эффективности маркетинга (UA, привлечение пользователей) Инструменты: Amplitude, Appsflyer, Facebook etc Есть маркетинговая аналитика, которая касается исследований рынка и так далее. Там совершенно иные требования к навыкам и задачи. Продуктовая аналитика что такое продукт Все, что может быть предложено на рынке с целью удовлетворения чьих-либо желаний и потребностей. В IT под продуктом обычно понимают приложение или какой-то функционал приложения. Соответственно, продуктовая аналитика — анализ того, как пользователи взаимодействуют с приложением и предложенным функционалом (и как за него платят). Близко к web-аналитике, но отличается более детальными данными про пользователя и его поведение, а не просто статистику страниц и переходов. CX/UX-исследователи Тоже близки к продуктовым аналитикам, но больше ориентированы на опыт пользователя и то, как он взаимодействует с приложением (интерфейс) и как использует приложение для решения своих задач. Основные методы – интервью, опросы, фокус-группы, UX-тесты и т.д. структура команды разработки Продуктовые аналитики тесно взаимодействуют с командами разработки (особенно если это мобильные приложения), в основном с продюсерами и разработчиками (особенно на этапе построения систем аналитики новых продуктов), с отделом маркетинга, существенно реже - с коммьюнити-менеджерами. продакт-менеджер / продюсер проджект-менеджер (PM) разработчики (клиент/сервер) дизайнеры (арт), UI аналитики тестировщики системные администраторы коммьюнити саппорт роли продуктовых аналитиков калькулятор интерфейс к базе данных специалист по дашбордам аналитик фич и апдейтов генератор идей / мастер на все руки Стадии развития продукта этапы жизни технические этапы Концепт Прототип Продукт, готовый к запуску Soft launch Global launch Оперирование Поддержка Чаще всего, конечно, продуктовые аналитики работают с продуктом в стадии оперирования - когда идет эволюционное развитие, постоянный приток новых пользователей и есть активная команда разработки. Основные задачи: анализ фич (функционала), контроль баланса, улучшение UX, поддержка продактов при проектировании новых фич. Также аналитики работают с продуктом, готовым к первому запуску и на этапах soft/global launch. Это периоды построения системы аналитики и тестирование, как пользователи реагируют в целом на продукт и на ключевой функционал продукта. Бизнес-модели компоненты все, что связано с разработкой и производством продукта все, что связано с продажей продукта, от поиска нужных клиентов до распространения продукта все, что связано с тем, как клиент будет платить и как компания будет зарабатывать варианты Subscription Model. Монетизация через подписку. Я.МУзыка, онлайн-кинотеатры и т. д. Freemium Model. Многие мобильные игры, все продукты с разными тарифными планами. Advertising Model. Продукты, которые получают деньги за счет показа рекламы. Социальные сети, Youtube и т. д. E-commerce/Marketplace Model. Магазины, маркетплейсы и классифайды. Transaction/Commission Model. Монетизация за счет комиссии. AirBnB, платежные системы типа Мир, Visa. On-Demand Model. Товар по требованию. Печать книг (Ridero), различный мерч. Licensing Model. Продажа лицензий. Microsoft Office. Pay-Per-Use Model. Плати и используй. Самокаты, облачные сервисы AWS. Crowdsourcing/Funding Model. Живут за счет донатов и пожертвований. Wikipedia. … тысячи их Полезные материалы рассказ Алексея Натекина про виды датасаентистов рассказ Валерия Бабушкина про то, почему датасаентист - очень общий термин Неплохая статья одного из аналитиков Яндекса. Его мысль про партизанской продакт-менеджмент наглядно описывает, какая роль аналитика в команде самая эффективная и, в общем-то, интересная. Хороший доклад про роли аналитиков в продуктовых (в первую очередь геймдев) командах. Немного многословно, но основные пункты освещены. фреймворк продуктовой аналитики, основанный на задачах на разных этапах развития продукта пара слов про бизнес-модели (я частично ориентировался на эти материалы): раз, два и три телеграм-канал про стартапы, любопытно посмотреть, какие вообще бывают идеи стартапов и на удовлетворение каких потребностей они ориентированы Домашнее задание Промежуточные задания необязательны и нужны для тех, кто хочет развивать свои навыки в области аналитики или в R/Python/SQL. О занятиях, которые будут оцениваться, я сообщу отдельно и не один раз. Обязательно! Настройте для себя привычную вам рабочую среду для работы с данными (R или Python). Если у вас возникнут какие-то затруднения с этим, напишите мне. В подчате #welcome напишите несколько слов о себе: какой опыт работы с R/Python/SQL и вообще языками программирования, есть ли опыт работы аналитиком (и где, если есть). Какие ожидания от курса, какие темы вам интереснее всего. подумайте и напишите мне, каких специалистов и из каких компаний вы хотели бы послушать (кого стоит попробовать пригласить). Не какие-то конкретные люди, а роли. посмотрите на ваши установленные приложения и подумайте, какие ваши потребности они реализуют попробуйте определить, как организован поток денег от вас к компании в ваших приложениях, за что вы платите и как (какая бизнес-модель используется в этом приложении) попробуйте определить самое интересное для себя приложение с точки зрения потребностей и их монетизации, чем оно вам оказалось интересным? Если это возможно, напишите, пожалуйста, свои размышления о приложениях в подчате #discussion нашего телеграм-канала. Если есть желание развиваться в сфере аналитики и продуктовой аналитики: напишите мне о своем желании в личку и скажите, какие навыки лично вы хотели бы подтянуть во время курса. поищите различные вакансии веб-аналитиков, продуктовых и маркетинговых аналитиков. Посмотрите требуемые основные навыки: что из этого вы уже умеете, чему хотели бы научиться, а чем даже понятия не имеете. Определите зону или траекторию своего развития. Если считаете, что я могу помочь вам с этим – напишите, попробуем. "],["метрики-вовлечения-pt1.html", "Метрики вовлечения pt1 Запись занятия AARRR фреймворк User Aquisition Активность и вовлечение Полезные материалы Домашнее задание", " Метрики вовлечения pt1 Запись занятия AARRR фреймворк User Aquisition Метрики привлечения пользователей в основном используются маркетинговыми аналитиками и специалистами по user aquisition, привлечению пользователей. Продуктовые аналитики в основном работают с метриками стоимости пользователя: CPA (cost per action), CPI (cost per install), хотя иметь представления о прочих метриках тоже надо. Процесс привлечения пользователей рекламодатель (тот, кто хочет привлечь пользователей) аукцион рекламной площадки (рекламная площадка выбирает, кому, когда и по какой цене показыть рекламные материалы) целевые действия (установка, платеж и т.д.) (в зависимости от того, на выполнение какого целевого действия оптимизируется рекламная сеть, в приложение будут приходить разные пользователи - те, кто вероятнее всего установит приложение/сделает платеж / сделает другое целевое действие) управление кампанией - таргетинг, бюджет, креативы (рычагов управления рекламными кампаниями не так уж и много - на кого ориентируем рекламу, какой бюджет в день рекламная сетка может потратить на привлечение пользователей, какие рекламные атериалы показываем) Маркетинговая воронка в мобильных приложениях Когда пользователи видят рекламу, они проваливаются в “воронку” — последовательность шагов, которые приводят пользователя в приложение. Вообще воронки — полезный инструмент для оценки, где и на каком этапе отваливается пользователь. реклама (баннер, playable, прочий креатив): CPM (cost per mille - сколько платим за каждые 1000 показов рекламных материалов), CPC (cost per click - сколько платим рекламной сетке за каждый клик по баннеру), СTR(click through rate – клики / показы) переход в стор установка приложения (CR, conversion rate, регистрации / показы) целевое действие (CPI, CPA) Для продуктовых аналитиков важнее всего стоимость пользователя (как правило, CPI или CPA), так как это позволяет сопоставить, сколько заплатил пользователь за время своей жизни в приложении и сколько потратили на его привлечение. То есть, окупился пользователь или нет. Оптимизировать окупаемость можно и с помощью понижения цены закупки, и с помощью повышения среднего чека / LTV пользователя. На последнее как раз влияют прождуктовые изменения, которые и входят в зону ответственности продуктовых аналитиков. Новый пользователь Основная цель рекламных кампаний – привлечение пользователей в приложение. Одна из базовых метрик этого процесса – количество новых пользователей. Однако есть сложности с самим определением, что такое новый пользователь. В частности, считать инсталлом новое физическое устройство. Или новый аккаунт пользователя. Или пользователя, который сделал покупку / подписку (e-comm, в частности). Например, когда пользователь на новый телефон устанавливает приложение и туда происходит логин с помощью его гугл/эппл аккаунта. В этом смысле устройство новое (а маркетинг закупает девайсы), а пользователь старый. Или когда пользователь пользовался приложением на телефоне. Потом удалил и через пару лет увидел рекламу и поставил заново и создал новый аккаунт. Или просто отдал телефон кому-то. Девайс старый, а пользователь новый. Другая история с новыми пользователями – это механизмы ретаргетинга. Это когда мы стараемся вернуть в приложение пользователей, которые уже были нашими пользователями, но потом отвалились. Например, мы выбираем набор девайсов с каким-то суммарным платежом более X единиц, и просим рекламной сетке именно этим устройствам показать нашу рекламу, с помощью которой мы надеемся вернуть пользователей. Этих пользователей сложно считать новыми, но рекламная кампания на них была и, соответственно, сколько-то мы потратили на возвращение этих пользователей. Активность и вовлечение Другая группа метрик - метрики активности и вовлечения пользователей в продукт. К этим метрикам относят обычно количество заходов пользователя в день (количество сессий), количество уникальных пользователей, заходящих в день в приложение. В некоторых случаях считают более длинные метрики - количество уникальных пользователей, зашедших в приложение в последнюю неделю/месяц. DAU, WAU, MAU Daily Active Users Weekly Active Users Monthly Active Users Основная метрика - DAU, как наиболее гибкая и быстро реагирующая на изменения в продукте. Месячные и недельные метрики считаются в скользящем окне, за последние 30 и 7 дней для каждой даты соответственно. Stickness / Sticky factor Иногда смотрят отношение DAU/MAU и интерпретируют как метрику залипания пользователя в проект, его лояльности. В целом это метрика вполне хорошо заменяется метриками удержания (retention). Retention rate Метрика удержания пользователя (retention) — какая доля пользователей вернулась в приложение. Во многом формула расчета ретеншена зависит от того, что мы считаем точкой отсчета. Когда речь идет о мобильных приложениях развлекательного плана (игры, стриминговые сервисы и проч.), то точкой отсчета обычно считают день инсталла, когда пользователь установил приложения. В некоторых продуктах может быть иначе, например, в e-commerce или в сервисах, предлагающих определенные услуги оффлайн (доставка продуктов), считаются только возвраты тех пользователей, которые сделали уже платеж (возвратом считается последующий платеж). В целом, метрика удержания одна из важнейших в аналитике - она позволяет понимать, насколько пользователям интересно приложение (сервис), останутся ли они в нем. Соответственно, это прямо влияет на монетизацию: когда пользователи остаются, они либо больше платят, либо, как минимум, есть шансы их побудить сделать платеж (скидками, новыми фичами и т.д.) Нюансы: install day = day 0: традиционно день инсталла считается нулевым днем. day 1/7/14/28: полезно иметь в виду, что бывают циклы, например, ретеншен в течение недели может варьировать в определенном диапазоне. Соответственно, сравнивать два периода/объекта/тестовых группы хорошо бы по одному и тому же по структуре интервалу. проблема интервала (сутки vs календарная дата): обычно считается ретеншен по календарным дням, то есть, если произошла смена даты, то это уже другой день, даже если пользователь установил приложение в 23.55. Временами встречаются вычисления ретеншена строго по 24 часовым интервалам (вернувшийся в игру через 24 часа). Метрики удержания по этим двум формулам вычисления различаются, всегда надо уточнять, как именно велся расчет. rolling retention: иногда нет возможности логировать каждый заход пользователя в приложение, поэтому используется только дата последнего захода пользователя в приложение - то есть, считается, какая доля пользователей заходила после N дня от инсталла. Иногда retention 1 дня / удержание 1 дня сокращают до ret1 / ret1d, r1 (номер дня может быть любым, не только 1). однородность когорт: когда мы считаем удержание по когорте пользователей (например, пришедшим в сентябре), то мы должны считать ретеншен только того дня, который могли прожить все пользователи. То есть, на момент 3 сентября нельзя считать ретеншен 7 дня для тех, кто пришел в приложение 31 августа - они принципиально не могли прожить 7 дней, максимум - 2 (день инсталла и 1-2 октября, 3 сентября также нельзя считать, так как день еще не закончился). Соответственно, по всей месячной когорте можно считать только ret2, даже для тех, кто пришел в начале сентября и мог провести в приложении больше дней. Иногда это минимальное количество дней, которые могли прожить пользователи всех когорт, называют окном лайфтайма. Churn rate Отвалы (churn, отток) - ситуация, когда пользователь окончательно уходит из приложения. Как правило, это достаточно определить, что пользователь больше не вернется, поэтому операционализируют в духе “отвалившийся пользователь - пользователь, который был неактивен последние N дней”. Также как и ретеншен, операционализация отвала может зависеть от приложения и сервиса. Стоить помнить, что отток не тождественен удержанию с другим знаком, хотя достаточно близок по смыслу. Sessions per day Еще одна метрика вовлеченности пользователя в продукт - сколько раз пользователь открывает приложение в течение дня. В более общем виде - какие-то значимые активные действия в единицу времени. Количество сессий в день можно интерпретировать как степень рутинизированности, включенности в повседневные практики пользователя. Для разных продуктов и сервисов, само собой, будут свои критерии - для игр жанра match3 нормально, если пользователь 4-6 раз в день открывает приложение. А вот для приложения оплаты штрафов или банковских приложений это была бы странная метрика, там вообще могут потребоваться другие способы измерения и вовлечения. Полезные материалы Статья про Rolling retention и Retention rate от Олега Якубенкова. Статья от dev2dev про Retention. Список фреймфорков, которые используют продуктовые менеджеры в своей работе. Оффтоп: cмешной случай, как потеряли информацию о 16 тысячах заболевших граждан. Хороший пример, почему для работы с большими данными (да и просто с данными) Excel не очень полезен. Домашнее задание Домашние занятия для желающих. Если будут вопросы, пишите в канал #discussion. Если возникнет необходимость получить от меня какие-то персональные комментарии - пишите в личку. Задание можете выполнять на любом доступном вам языке / среде для статистики. level 1 (IATYTD) Обновите знания по работе с табличками — аггрегации (групировки), слияния, создание и модификация колонок. Ссылка на конспекты прошлого курса: https://mar231s.upravitelev.info/ level 2 (HNTR) Необходимо подсчитать и нарисовать, сколько пользователей в день приходит в приложение, в том числе и с разбивкой по платформам. Датасет: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv level 3 (HMP) Используя датасет по заходам пользователей в приложение (dau.csv), подсчитайте и отобразите на графике, сколько пользователей в день заходит в приложение (DAU). Ссылка на файл. Осторожно, файл около 400мб. level 4 (UV) На основе данных по логинам нарисуйте area plot DAU проекта, в котором цветами выделите группы пользователей по количеству дней с момента инсталла: группа 1: 0 дней с инсталла группа 2: 1-7 дней с момента инсталла группа 3: 8-28 дней с инсталла группа 4: более 28 дней с инсталла У вас должно получится что-то вроде слоеного пирога, где цветами выделены группы. Подумайте, есть ли необходимость рисовать этот график не в абсолютных числах (количество пользователей), а в долях каждой группы от DAU, в чем могут быть плюсы и минусы такого графика. Возможно, вам потребуется нарисовать графики разных типов, чтобы ответить на этот вопрос. Попробуйте подумать, что говорит подобный график о продукте и его пользователях. Есть ли у него проблемные зоны, над которыми надо поработать или которые могут влиять на стратегию развития и/или оперирования продукта? level 5 (N) Постройте графики DAU, MAU и их отношения для данных за июль. Проинтерпретируйте метрику DAU/MAU, что она говорит о проекте? "],["метрики-вовлечения-pt2.html", "Метрики вовлечения pt2 Запись занятия Код занятия на Python Разбор домашнего задания Расчет retention Домашнее задание", " Метрики вовлечения pt2 Запись занятия Код занятия на Python https://colab.research.google.com/drive/1FcFTz7sI8QXhVLcLnshyHFMdGS5znlw5?usp=sharing Разбор домашнего задания level 2 (HNTR) Необходимо подсчитать и нарисовать, сколько пользователей в день приходит в приложение, в том числе и с разбивкой по платформам. Датасет: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv Решение: # подключаем пакеты (они до этого должны быть установлены) library(data.table) library(plotly) # если есть ошибка с %&gt;%, то явно подключаем соответствующий пакет library(magrittr) # импортируем данные # installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) installs &lt;- fread(&#39;./data/installs.csv&#39;) # считаем количество уникальных пользователей по дням intalls_stat &lt;- installs[, list(n_users = uniqueN(user_pseudo_id)), by = list(dt, media_source)] # сортируем по дате инсталла intalls_stat &lt;- intalls_stat[order(dt)] # рисуем график plot_ly(intalls_stat, x = ~dt, y = ~n_users, color = ~media_source, type = &#39;scatter&#39;, mode = &#39;none&#39;, stackgroup = &#39;one&#39;) %&gt;% layout( title = &#39;Установки приложения по дням&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) level 4 (UV) На основе данных по логинам нарисуйте area plot DAU проекта, в котором цветами выделите группы пользователей по количеству дней с момента инсталла: группа 1: 0 дней с инсталла группа 2: 1-7 дней с момента инсталла группа 3: 8-28 дней с инсталла группа 4: более 28 дней с инсталла У вас должно получится что-то вроде слоеного пирога, где цветами выделены группы. Подумайте, есть ли необходимость рисовать этот график не в абсолютных числах (количество пользователей), а в долях каждой группы от DAU, в чем могут быть плюсы и минусы такого графика. Возможно, вам потребуется нарисовать графики разных типов, чтобы ответить на этот вопрос. Попробуйте подумать, что говорит подобный график о продукте и его пользователях. Есть ли у него проблемные зоны, над которыми надо поработать или которые могут влиять на стратегию развития и/или оперирования продукта? Решение: # если падает по таймауту options(timeout=360) # импортируем датасает # dau &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv&#39;) dau &lt;- fread(&#39;./data/logins.csv&#39;) # считаем количество дней от инсталла dau[, lifetime := login_dt - install_dt] # делим на группы dau[, lifetime_group := cut(lifetime, breaks = c(-Inf, -1, 0, 7, 28, Inf), ordered_result = TRUE)] # если хотим перезадать порядок уровней # dau[, lifetime_group_ := factor(lifetime_group, # levels = c(&#39;(-1,0]&#39;, # &#39;(28, Inf]&#39;, &#39;(0,7]&#39;, # &#39;(7,28]&#39;, &#39;(-Inf,-1]&#39;))] # второй способ разметить группы # dau[lifetime == 0, lifetime_group_3 := &#39;0. 0 day&#39;] # dau[lifetime &gt;= 1 &amp; lifetime &lt;= 7, lifetime_group_3 := &#39;1. 1-7 days&#39;] # dau[lifetime &gt;= 8 &amp; lifetime &lt;= 28, lifetime_group_3 := &#39;2. 8-28 days&#39;] # dau[lifetime &gt;= 28 &amp; lifetime &lt;= 90, lifetime_group_3 := &#39;3. 28+ days&#39;] # создаем отдельную группу для тех, про кого мы не знаем # dau[is.na(lifetime_group), lifetime_group_3 := &#39;unknown&#39;] # третий метод, с помощью fcase # dau[, lifetime_group := fcase( # lifetime == 0, &#39;0 дней&#39;, # lifetime &gt;= 1 &amp; lifetime &lt;= 7, &#39;1-7 дней&#39; # )] # считаем DAU dau_stat &lt;- dau[, list(n_users = uniqueN(user_pseudo_id)), keyby = list(login_dt, lifetime_group)] dau_stat[, total_users := sum(n_users), by = login_dt] dau_stat[, share := n_users / total_users] # area-plot plot_ly(dau_stat, x = ~login_dt, y = ~n_users, color = ~lifetime_group, type = &#39;scatter&#39;, mode = &#39;none&#39;, stackgroup = &#39;one&#39;) %&gt;% layout( title = &#39;DAU по группам пользователей&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) # график линиями plot_ly(dau_stat, x = ~login_dt, y = ~n_users, color = ~lifetime_group, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;DAU по группам пользователей&#39;, xaxis = list(title = &#39;&#39;), yaxis = list(title = &#39;&#39;, rangemode = &#39;tozero&#39;)) %&gt;% config(displayModeBar = FALSE) level 5 (N) Постройте графики DAU, MAU и их отношения для данных за июль. Проинтерпретируйте метрику DAU/MAU, что она говорит о проекте? Решение. Строим график MAU. # берем интересующие нас дни dates &lt;- dau[login_dt &gt;= &#39;2022-07-01&#39;, sort(unique(login_dt))] # проходим циклом lapply mau_stat &lt;- lapply(dates[1:2], function(x) { # берем данные в интервале &quot;наша дата - 30 дней -- наша дата&quot; result &lt;- dau[login_dt &gt;= x - 30 &amp; login_dt &lt;= x] # считаем, сколько пользователей заходило за это время (mau) result &lt;- result[, list(dt = x, dt_lb = x - 30, mau = uniqueN(user_pseudo_id))] result }) # собираем все в табличку mau_stat &lt;- rbindlist(mau_stat) # аналогичное решение, более современное по функциям # + считаем одновременно dau и mau library(purrr) mau_stat &lt;- map_df(dates, function(x) { result &lt;- dau[, list( dt = x, dt_lb = x - 30, metric_dau = uniqueN(user_pseudo_id[login_dt == x]), metric_mau = uniqueN(user_pseudo_id[login_dt &gt;= x - 30 &amp; login_dt &lt;= x]) )] result }) setDT(mau_stat) # считаем stickiness mau_stat[, stickiness := metric_dau / metric_mau] # рисуем DAU и MAU plot_ly(mau_stat, x = ~dt, y = ~metric_mau, type = &#39;scatter&#39;, mode = &#39;lines&#39;, name = &#39;MAU&#39;) %&gt;% add_trace(y = ~metric_dau, name = &#39;DAU&#39;) %&gt;% layout( title = &#39;DAU и MAU&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) # рисуем stickiness plot_ly(mau_stat, x = ~dt, y = ~stickiness, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;DAU / MAU&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Расчет retention Общая логика расчета: - считаем lifetime - считаем количество пользователей на каждый день от инсталла - считаем долю этих пользователей от всего пользователей когорты - ограничиваем на общий доступный лайфтайм - рисуем график - опционально – добавляем группировку # берем только тех, кто пришел в июне retention &lt;- dau[install_dt &gt;= &#39;2022-06-01&#39;] retention &lt;- retention[install_dt &lt; &#39;2022-07-01&#39;] # ограничиваем на минимальное общее количество дней retention &lt;- retention[lifetime &lt;= 30 &amp; lifetime &gt;= 0] # считаем количество вернувшихся retention_stat &lt;- retention[, list(returned = uniqueN(user_pseudo_id)), keyby = list(platform, lifetime)] # считаем,с колько всего было retention_stat[, total_users := returned[lifetime == 0], by = platform] # второй вариант расчета total_users, через merge retention_stat &lt;- merge( retention_stat, retention_stat[lifetime == 0, list(platform, total_users_2 = returned)], by = &#39;platform&#39;, all.x = TRUE ) # считаем retention retention_stat[, ret := returned / total_users] # рисуем график plot_ly(retention_stat, x = ~lifetime, y = ~ret, color = ~platform, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Retention rate&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Домашнее задание level 1 (IATYTD) Внимательно разберите решения заданий (материалы конспекта). level 2 (HNTR) Постройте график ретеншена для когорты пользователей, пришедшей в июне, с разбивкой по источникам привлечения (media_source). Для этого вам потребуются следующие датасеты: Инсталлы: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv Логины: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv level 3 (HMP) Постройте линейный график retention 1 day (ret1) для всех дневных когорт. Т.е. по оси OX должна быть дата инсталла, по оси OY – значение ретеншена первого для пользователей, пришедших в этот день. level 4 (UV) Добавьте на этот график группировку по источникам трафика (media_source). level 5 (N) Постройте и сравните графики rolling retention и retention rate (возьмите данные за логины и инсталлы из практикума). Для rolling retention необходимо: посчитать максимальный лайфтайм пользователя посчитать количество пользователей по лайфтайму cделать обратную кумулятивную сумму cумму поделить на количество установок (для lifetime == 0 значения количества инсталлов и обратная кумсумма должны совпадать) "],["метрики-монетизации-pt1.html", "Метрики монетизации pt1 Запись занятия Код занятия на Python Разбор домашнего задания Метрики монетизации Домашнее задание", " Метрики монетизации pt1 Запись занятия Код занятия на Python https://colab.research.google.com/drive/1gdtpFd3LjzRSsFwPrKP-OARjDXDU3F1v?usp=sharing Разбор домашнего задания level 2 (HNTR) Постройте график ретеншена для когорты пользователей, пришедшей в июне, с разбивкой по источникам привлечения (media_source). Для этого вам потребуются следующие датасеты: Инсталлы: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv Логины: https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv library(data.table) library(plotly) ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout # чтобы загрузка не обрывалась по таймауту, если слабый интернет options(timeout=360) # installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) installs &lt;- fread(&#39;./data/installs.csv&#39;) # logins &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv&#39;) logins &lt;- fread(&#39;./data/logins.csv&#39;) # выделяем инсталлы в июне installs_june &lt;- installs[dt &gt;= &#39;2022-06-01&#39; &amp; dt &lt; &#39;2022-07-01&#39;] # считаем количество пользователей по платформам installs_june[, uniqueN(user_pseudo_id), keyby = media_source] ## Key: &lt;media_source&gt; ## media_source V1 ## &lt;char&gt; &lt;int&gt; ## 1: 36169 ## 2: Facebook Ads 1297 ## 3: applovin_int 36714 ## 4: googleadwords_int 7767 ## 5: organic 32 ## 6: other 6869 ## 7: unityads_int 21932 # при необходимости укрупняем installs_june[media_source %in% c(&#39;organic&#39;, &#39;other&#39;) | is.na(media_source), media_source := &#39;organic&#39;] # смотрим, нет ли тех, кто два раза устанавливал приложение installs_june[, list(n_install_dates = length(dt)), user_pseudo_id][n_install_dates &gt; 1] ## user_pseudo_id n_install_dates ## &lt;char&gt; &lt;int&gt; ## 1: 000ee53a25d9b57b29a8513070325bba 2 ## 2: 0114C14103154E4B8202B09684ECBC1F 2 ## 3: 01958D392F90428AA0DAF9D94CC699DE 2 ## 4: 09B886173CAF46E7B99C2927CE954E9B 2 ## 5: 0DA2A73360594B5D94C77489A563C9C9 2 ## 6: 1A041DCE27EC4DE984379C91778615E1 2 ## 7: 1CB08A3B991A41848CF193759534A8E3 2 ## 8: 1FC5CCB7EAA34A1CA4D4FB9731AAD6B4 2 ## 9: 2488537605DD48DC89CB9AE2A7F056DC 2 ## 10: 2585F39E330445CCB04D86C8874E04AE 2 ## 11: 2E7FE3A85A434C19AE66C5F5A9909488 3 ## 12: 30B149EB924045D7B710BB0ECCD3E3C8 2 ## 13: 330704BC3B27431B910C5EFF17F7E224 2 ## 14: 34cd9bf8909ccd8ff7845e57374284d1 2 ## 15: 369FD7EBC865452CB1B33E5E98AC447C 2 ## 16: 36D082CC8AB7447683877B1AFED0EDDB 2 ## 17: 382548ABD43A4CC3956408AA15065DD5 2 ## 18: 3B22021A23624273B976BB68F7BC133E 2 ## 19: 3B41BA33DDCA463180F1288B1FDAB9E5 2 ## 20: 3CFDCBE58131436483C5DB672E0C8BA5 2 ## 21: 3E5DBDEB38FE408FA281C75B5B33D08A 2 ## 22: 41E59CD840224F4286041CF23454D1A5 2 ## 23: 42F53713F7B04EF8ADA81A51EDFAA0FC 2 ## 24: 453D178E341A45409852BFBEF7668FCA 2 ## 25: 4df8d48b980fb52e7143b8f4f6a86eec 2 ## 26: 53d5883b315f11beb96e759eeb6c6946 2 ## 27: 541291B4FC0A472C8A18F33FCBB578E7 2 ## 28: 55734100B47F414CBB3575028EC0E887 2 ## 29: 58E14628B03046219960479EA035BAC4 2 ## 30: 5f86721f15e1fe055162a1251f2f8b01 2 ## 31: 67A5140641B44E26B9F9699A767EAA1B 2 ## 32: 696f7a53d58e73d2a1b9c5ff841741d6 2 ## 33: 6F22E223CB3D4DEF9CE5F64073099A8C 3 ## 34: 7044781DFEC247C79D9641EFB09CDF79 2 ## 35: 78264f9fd5ab87d279dc8f7c98279f55 2 ## 36: 79f6234ccd8fc0ccfcd4058a9d4173f6 2 ## 37: 7F25AD2C389542048506DF670BD714BD 2 ## 38: 8159da026a6e5393b11783bb56df4c14 2 ## 39: 822abdbc7bef93fbe985f4e89651dd75 2 ## 40: 83F7F0494B2F4D49B934C12A92B4B299 2 ## 41: 84e435c0c8689635cf96ceeabf52c534 2 ## 42: 854366FB9D794A83B85B666A808DF8C2 2 ## 43: 8C03EE3FBBFC472BB26DB331567DAA6A 2 ## 44: 8FD70E7743B74FDA8B398B8E59B6D393 2 ## 45: 908C0586AFD14790A2FEAD8380D5E5AC 2 ## 46: 9655A5286F2D4782BB95B97C4443AB19 2 ## 47: 98AAE0DB60B44B649FB45DAFF50E5756 2 ## 48: 9B1CAF9AB0B2438A92D7FB377BDF1CD5 2 ## 49: 9DC9EC1BE76D48AD9161A8FDA1AD8F5D 2 ## 50: 9EBF8F8182D84FBD99DE634AC3467F7E 2 ## 51: A1084F73187648EFAAD54B3FC01A25EF 2 ## 52: A3DD0C1CACA4442085DBFFC493DBEFDE 2 ## 53: A7072DB51E414373B45AF4495BE5E30D 2 ## 54: ABD07667D3124C969E098EED5FFD25B2 2 ## 55: AE7CF2B145C94D94A2782A4986A56C7A 2 ## 56: BA8BE90B36464ED48B1F6596C8B9218A 2 ## 57: BCF666CE7D004703B1C7A998E0029B3D 2 ## 58: BD2FC2B3E09441AAA0C8DAAF079DFEE7 2 ## 59: BEE44FD3EB6B464FB19E89B3DD18C1D8 2 ## 60: CB97620A41514DD6A572406AC15DA962 3 ## 61: CEB5B07C3DE244B4B042D7663C4957F2 2 ## 62: D367DC87B0AF47CEB44219CB5C16B136 2 ## 63: D8DD91C418E644E6BD3A2A46DE12DD86 3 ## 64: DF4612D16A6C49A08C1C7F64B43ADCF0 2 ## 65: E5E18D1017294B0992C09A454C9519A8 2 ## 66: ECDC46825D71492794912D761A78A71F 2 ## 67: EF4084E36ABA4A889CC21272B1791355 2 ## 68: F2FB00FFF2C74EA4B8BC87992CED3B6A 2 ## 69: F559AD32A74E411C9B60873CCC3614A1 2 ## 70: F73BD84C3713442187AF3289DA2D2172 2 ## 71: FEE12ECCA2E94760909C8F64EFEF066B 2 ## 72: FFFD930C3F9F4AB7B9786983044292FE 2 ## 73: a43d9e6b9741ccb23d0138d3d7ba325a 2 ## 74: a92b088adfb0f9374dcfec4f884c76d9 2 ## 75: add417438334536d4e41fee42cecbeac 2 ## 76: b35107c77a6c0a3e5e55dc980c0c435c 2 ## 77: b8b9848ce34f9661eae6b471522d5265 2 ## 78: b929015e9c4eeee3e1d4a7e6a22b39d2 2 ## 79: cf38d8f4f997cc7a2bde1cf6b9636a51 2 ## 80: d101bd547e90e2a43ffbe29cdc0d7c6a 2 ## 81: de47c1adf204ee72a6745901052a26ae 2 ## 82: df7e04c01ef23152ce370e6655de6133 2 ## 83: ee01a31f793d9200e110ba607747d2d2 2 ## 84: f0d4270e629a31d5f73ba47d6fb8de07 2 ## 85: f2cd8d5642834168fa5dcfeb42a4b5d6 2 ## 86: f43c58ff000c5ba637bb622848c2597b 2 ## 87: fa6827f8ebd2805fe69ac2fcdafbe18a 2 ## 88: fd72559f6d926b4e770290d48fb422e5 2 ## user_pseudo_id n_install_dates users_reinstalls &lt;- installs_june[, list(n_install_dates = length(dt)), user_pseudo_id][n_install_dates &gt; 1, unique(user_pseudo_id)] # чистим таких пользователей # installs_june &lt;- installs_june[!user_pseudo_id %in% users_reinstalls] # можно удалить, можно взять первый инсталл # installs_june &lt;- installs_june[order(user_pseudo_id, dt)] # installs_june &lt;- installs_june[, .SD[1], by = user_pseudo_id] # присоединяем к установкам логины installs_june &lt;- merge( installs_june, logins, by = c(&#39;user_pseudo_id&#39;, &#39;platform&#39;), all.x = TRUE ) # вычисляем лайфтайм installs_june[, lifetime := login_dt - dt] # ищем пользователей, у которых логин был раньше даты инсталла installs_june[login_dt &lt; dt, uniqueN(user_pseudo_id)] ## [1] 204 # чистим таких пользователей installs_june &lt;- installs_june[!user_pseudo_id %in% installs_june[login_dt &lt; dt, unique(user_pseudo_id)]] # еще одна чистка, для красоты -- берем только тех пользователей, у которых есть lifetime = 0 installs_june &lt;- installs_june[user_pseudo_id %in% installs_june[lifetime == 0, unique(user_pseudo_id)]] # считаем количество вернувшихся на кайждый день лайфтайма installs_june_stat &lt;- installs_june[, list(returned = uniqueN(user_pseudo_id)), by = list(media_source, lifetime)] # так как выше почистили путешественников во времени, эта фильтрация избыточна installs_june_stat &lt;- installs_june_stat[lifetime &gt;= 0] # считаем количество инсталлов, data.table-way installs_june_stat[, total_users := returned[lifetime == 0], by = media_source] # # тоже количество инсталлов, староверы/python-way # installs_june_stat &lt;- merge( # installs_june_stat, # installs_june_stat[lifetime == 0, list(media_source, total_users = returned)], # by = &#39;media_source&#39;, all.x = TRUE # ) # считаем собственно ретеншен installs_june_stat[, ret := returned / total_users] # сортируем для красоты installs_june_stat &lt;- installs_june_stat[order(lifetime)] # рисуем график, попутно накладываем ограничение на количество дней лайтайма plot_ly(installs_june_stat[lifetime &lt;= 30], x = ~lifetime, y = ~ret, color = ~media_source, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Ретеншен июньской когорты в зависимости от источников трафика&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 3 (HMP) Постройте линейный график retention 1 day (ret1) для всех дневных когорт. Т.е. по оси OX должна быть дата инсталла, по оси OY – значение ретеншена первого для пользователей, пришедших в этот день. daily_ret &lt;- merge( installs, logins, by = c(&#39;user_pseudo_id&#39;, &#39;platform&#39;), all.x = TRUE ) # считаем лайфтайм daily_ret[, lifetime := login_dt - dt] # ищем пользователей, у которых логин был раньше даты инсталла daily_ret[login_dt &lt; dt, uniqueN(user_pseudo_id)] ## [1] 444 # чистим таких пользователей daily_ret &lt;- daily_ret[!user_pseudo_id %in% daily_ret[login_dt &lt; dt, unique(user_pseudo_id)]] # еще одна чистка, для красоты -- берем только тех пользователей, у которых есть lifetime = 0 daily_ret &lt;- daily_ret[user_pseudo_id %in% daily_ret[lifetime == 0, unique(user_pseudo_id)]] # считаем, сколько вернулось в каждой дневной когорте по дням от инсталла daily_ret_stat &lt;- daily_ret[, list(returned = uniqueN(user_pseudo_id)), by = list(dt, login_dt, lifetime)] # из-за нашей чистки странных пользователей у нас пропали некоторые даты с lifetime = 0 # для красоты их можно восстановить, но необязательно # считаем количество всего пользователей в когорте # daily_ret_stat[, total_users := returned[lifetime == 0], by = dt] daily_ret_stat &lt;- merge( daily_ret_stat[lifetime == 0, list(dt, total_users = returned)], daily_ret_stat, by = &#39;dt&#39;, all.x = TRUE ) # считаем ретеншен daily_ret_stat[, ret := returned / total_users] # сортируем daily_ret_stat &lt;- daily_ret_stat[order(dt, lifetime)] # daily_ret_stat &lt;- daily_ret_stat[!dt %in% c(&#39;2022-06-25&#39;, &#39;2022-07-06&#39;, &#39;2022-07-20&#39;)] # рисуем простой график # plot_ly(daily_ret_stat[lifetime == 1], # x = ~dt, y = ~ret, # type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% # layout( # yaxis=list(rangemode = &#39;tozero&#39;) # ) # чтобы сортирвока линий была нормальная ret_days &lt;- c(1, 3, 7, 14, 30) daily_ret_stat_plot &lt;- daily_ret_stat[lifetime %in% ret_days] daily_ret_stat_plot[, lifetime_cat := factor(lifetime, levels = ret_days)] # рисуем график ретеншена дневных когорт plot_ly(daily_ret_stat_plot, x = ~dt, y = ~ret, color = ~lifetime_cat, type = &#39;scatter&#39;, mode = &#39;lines&#39;) %&gt;% layout( title = &#39;Retention rate дневных когорт&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 5 (N) Постройте и сравните графики rolling retention и retention rate (возьмите данные за логины и инсталлы из практикума). # сначала считаем просто ret1 по всей июньской когорте, без разбивки по каналм привлечения # оставляем группировку только по дням лайфтайма retention_type &lt;- installs_june[, list(returned = uniqueN(user_pseudo_id)), by = list(lifetime)] # считаем количество инсталлов, data.table-way retention_type[, total_users := returned[lifetime == 0]] # считаем собственно ретеншен retention_type[, ret_rate := returned / total_users] Для rolling retention необходимо: посчитать максимальный лайфтайм пользователя посчитать количество пользователей по лайфтайму cделать обратную кумулятивную сумму cумму поделить на количество установок (для lifetime == 0 значения количества инсталлов и обратная кумсумма должны совпадать) # считаем rolling retention # считаем количество пользователей, в зависимости от того, на какой максимальный день от логина они вернулись rrolling &lt;- installs_june[, list(lifetime = max(lifetime)), by = list(user_pseudo_id, dt)] # считаем количество дней от инсталла до последнего логина rrolling_stat &lt;- rrolling[, list(n_users = uniqueN(user_pseudo_id)), keyby = lifetime] # нужна обратная кумулята, так как мы считаем &quot;сколько пришло после дня x&quot; # а в статистике у нас &quot;сколько пришло в день x&quot; - то есть, для каждого дня надо получить, # накопительную сумму этого и всех следующих дней. а это делается с помощью обратной кумуляты # для этого мы переворачиваем значения колонки с помощью rev(), считаем обычную кумуляту # а потом результат переворачиваем обратно # чтобы понять результат, попробуйте выражения: 1:5; rev(1:5), cumsum(1:5), cumsum(rev(1:5)), rev(cumsum(rev(1:5))) rrolling_stat[, returned_after := rev(cumsum(rev(n_users)))] # или более простой вариант rrolling_stat = rrolling_stat[order(-lifetime)] rrolling_stat[, returned_after_2 := cumsum(n_users)] rrolling_stat = rrolling_stat[order(-lifetime)] # проверяем rrolling_stat[, all.equal(returned_after, returned_after_2)] ## [1] TRUE # объединяем retention_type &lt;- merge( retention_type, rrolling_stat, by = &#39;lifetime&#39;, all.x = TRUE ) retention_type[, ret_rolling := returned_after / total_users] retention_type &lt;- retention_type[lifetime &lt;= 30] # и рисуем plot_ly(retention_type, x = ~lifetime, y = ~ret_rate, type = &#39;scatter&#39;, mode = &#39;lines&#39;, name = &#39;Retention rate&#39;) %&gt;% add_trace(y = ~ret_rolling, name = &#39;Rolling retention&#39;) %&gt;% layout( title = &#39;Retention rate vs Rolling retention, июньская когорта&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Метрики монетизации Gross / Net Gross - общая сумма всех платежей. Обычно полезно для финансистов и прочей отчетности. Net (revenue) - сумма платежей после вычета налогов и комиссии магазина приложений. Полезно для вычисления метрик ARPU/ARPPU и их сравнения со стоимостью закупки пользователей (т. е. для оценки юнит-экономики и окупаемости проекта). Конверсия Обычно под конверсией понимают ситуацию, когда пользователь меняет один статус на другой. Например, становится из неплатящего платящим пользователем (совершает платеж). Нередко говорят “конверсия в корзину” и подобно – то есть, какая доля пользователей после просмотра товаров перешла в корзину (готова сделать платеж). Конверсия в платящих (здесь и далее мы говорим про это) считается по такой формуле: Conversion = N Paying Users / N Users Практикум Посчитать, какая доля пользователей стала платящими в интервале 7 дней о инсталла, должно получиться 1 число. Игнорируйте install_dt в табличке payments, используйте dt из installs payments &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv&#39;) # рисоединяем к инсталлам платежи conversion &lt;- merge( installs, payments, by = c(&#39;user_pseudo_id&#39;, &#39;platform&#39;), all.x = TRUE ) # считаем лайфтайм conversion[, lifetime := pay_dt - dt] # так как хотим считать метрику на 7 день от инсталла, то мы должны проконтролировать # что все пользователи могли прожить столько дней в приложении # поэтому оставляем инсталлы только по 2022-07-24 conversion &lt;- conversion[dt &lt;= &#39;2022-07-24&#39;] # совершенно примитивным способом считаем, сколько было пользователей, кто сделал платеж # до седьмого дня от инсталла включительно # при этом сюда не попадут пользователи, которые не делали платежей, так как сравнение lifetime &lt;= 7 # отсекает значения, когда lifetime is null (нет значений, так как не платил и нет дату платежа) conversion[lifetime &lt;= 7, uniqueN(user_pseudo_id)] / conversion[, uniqueN(user_pseudo_id)] ## [1] 0.01866886 # синтаксически другой способ расчета цифры, более data.table-way conversion[, uniqueN(user_pseudo_id[lifetime &lt;= 7]) / uniqueN(user_pseudo_id)] ## [1] 0.01867702 Домашнее задание level 1 (IATYTD) Внимательно разберите решения заданий (материалы конспекта). level 2 (HNTR) На основе данных по платежам нарисуйте area plot подневную структуру гросса проекта, в котором цветами выделите группы пользователей по количеству дней с момента инсталла: группа 1: 0 дней с инсталла группа 2: 1-7 дней с момента инсталла группа 3: 8-28 дней с инсталла группа 4: более 28 дней с инсталла Решение аналогично такому же заданию на расчет структуры DAU. level 3 (HMP) Посчитайте по каждой платформе конверсию в платящих в день инсталла. Когорта – пришедшие в июне. Делать аналогично динамике ретеншена первого дня. level 4 (UV) Постройте график накопительной конверсии для пользователей, кто пришел в июне. Для этого надо сначала посчитать, в какой день от инсталла пользователь сделал платеж (lifetime. Потом посчитать, сколько пользователей сделало первый платеж в 0-30 дни от инсталла (new payers). Посчитать накопительную сумму по количеству пользователей (cumulative new payers). Посчитать отношение cumulative new users / total users Нарисовать график. level 5 (N) Постройте график накопительной конверсии в когорте июньскийх пользователей с разбивкой по источнику пользователей. "],["c5_monetization.html", "Метрики монетизации pt.2 Запись занятия Код занятия на Python Разбор домашнего задания ARPU / ARPPU Полезные материалы Домашнее задание", " Метрики монетизации pt.2 Запись занятия Код занятия на Python https://colab.research.google.com/drive/1tAAkLUsyck3OJE-994owPh3TIGJI_qDX?usp=sharing Разбор домашнего задания level 4 (UV) Постройте график накопительной конверсии с разбивкой по источнику пользователей. library(data.table) library(plotly) # импортируем данные # installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) installs &lt;- fread(&#39;./data/installs.csv&#39;) # payments &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv&#39;) payments &lt;- fread(&#39;./data/payments.csv&#39;) # к платежам присоединяем дату инсталла и источник трафика conversion &lt;- merge( payments, installs[, list(user_pseudo_id, media_source, dt)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # берем июньскую когорту и ограничиваем даты лайфтайма conversion[, lifetime := pay_dt - dt] conversion &lt;- conversion[dt &lt; &#39;2022-07-01&#39;] conversion &lt;- conversion[lifetime &gt;= 0 &amp; lifetime &lt;= 30] # беру минимальный день от инсталла # это будет день первого платежа conversion_stat &lt;- conversion[, list(lifetime = min(lifetime)), by = list(user_pseudo_id, media_source)] # считаю количество пользователей, которые сделали платеж на этот день conversion_stat &lt;- conversion_stat[, list(new_payers = uniqueN(user_pseudo_id)), by = list(lifetime)] # считаю, сколько всего было пользователей в когорте conversion_stat[, total_users := installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] # сортирую и считаю накопительное количество пользователей, которые сделали # первый платеж в этот день от инсталла conversion_stat &lt;- conversion_stat[order(lifetime)] conversion_stat[, new_payers_cum := cumsum(new_payers)] # считаю накопительную конверсию в платящих conversion_stat[, cum_conversion := new_payers_cum / total_users] # рисую plot_ly( conversion_stat, x = ~lifetime, y = ~cum_conversion, type = &#39;scatter&#39;, mode=&#39;lines&#39; ) %&gt;% layout( title = &#39;Накопительная конверсия&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) ARPU / ARPPU Averange revenue per user - сумма платежей за определенный период, деленная на общее количество пользователей когорты. Средний чек, наверное, одна из самых важных метрик для оперирования продуктом, так как изучение структуры ARPU позволяет понять, за что платят пользователи и как можно улучшить эту метрику и так далее. Average revenue per paying user - сумма платежей за определенный период, деленная на количество платящих пользователей когорты. Обе метрики считаются в определенном окне (количестве дней от инсталла) - обычно 7, 28 или 30 дней. Это необходимо для того, чтобы учесть ситуацию, когда пользователи одной когорты (месячной, например) могли прожить разное количество дней в приложении. Или когда необходимо сравнить разные каналы привлечения, рекламные кампании или группы аб-тестов. Для оценки динамики метрики и приянтия продуктовых решений (на какой день от инсталла что-то сломалось) часто рисуют кривую кумулятивного ARPU. # расчет во многом похож на накопительную конверсию, так как тоже считается накопительно # но не количество пользователей, сделавших первый платеж, а просто выручка # повторяющийся блок # к платежам присоединяем дату инсталла и источник трафика conversion &lt;- merge( payments, installs[, list(user_pseudo_id, media_source, dt)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # берем июньскую когорту и ограничиваем даты лайфтайма conversion[, lifetime := pay_dt - dt] conversion &lt;- conversion[dt &lt; &#39;2022-07-01&#39;] conversion &lt;- conversion[lifetime &gt;= 0 &amp; lifetime &lt;= 30] # считаем выручку по дням от инсталла arpu_stat &lt;- conversion[, list(gross = sum(gross)), by = list(lifetime)] # считаем количество уникальных пользователей в июньской когорте arpu_stat[, total_users := installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] # сортируем и считаем кумулятивную выручку arpu_stat &lt;- arpu_stat[order(lifetime)] arpu_stat[, gross_cum := cumsum(gross)] # считаем кумулятивное ARPU arpu_stat[, cARPU := gross_cum / total_users] # рисуем plot_ly( arpu_stat, x = ~lifetime, y = ~cARPU, type = &#39;scatter&#39;, mode=&#39;lines&#39; ) %&gt;% layout( title = &#39;Cumulative ARPU, installs in June&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Полезные материалы Основные метрики мобильных приложений Очень обзорный материал от devtodev. Есть неплохой блок по метрикам монетизации. Домашнее задание level 1 (IATYTD) Внимательно разберите материалы конспекта. level 2 (HNTR) Постройте график накопительной конверсии в когорте июньских пользователей с разбивкой по источнику пользователей. level 3 (N) Посчитайте по каждой платформе динамику метрики конверсию в платящих в день инсталла. На графике на оси OX должна быть дата инсталла, на оси OY – значение конверсии в день инсталла. Делать аналогично динамике ретеншена первого дня. level 4 (UV) Постройте и нарисуйте структуру накопительного ARPU для июньских пользователей в зависимости оттого, какие offer_type покупали пользователи. Таким образом мы можем понять, какая товарная категория делает наибольший вклад в кумулятивное ARPU. ИЛИ Постройте график накопительного ARPU в когорте июньских пользователей с разбивкой по источнику пользователей. level 5 (N) Посчитайте по каждой платформе динамику ARPU 0, 1, 7 и 30 дней (сколько в среднем заплатили пользователи когорты в день инсталла, за 0 и 1 дни жизни в приложении, за первые 7 дней жизни, за первые 30 дней жизни в приложении). На графике на оси OX должна быть дата инсталла, на оси OY – значение ARPU, с разбивкой, по какому количеству дней от инсталла мы это считаем Делать аналогично динамике ретеншена, я показывал на занятии про ретеншен как раз близкое решение. "],["c6_monetization.html", "Метрики монетизации pt.3 Запись занятия Код занятия на Python Разбор домашнего задания Метрики монетизации ARPDAU Paying share Воронка платежей Домашнее задание", " Метрики монетизации pt.3 Запись занятия Код занятия на Python https://colab.research.google.com/drive/1P36fEEJQHb-q5yKR9ieywopqd2USJ39b Разбор домашнего задания Датасеты: library(data.table) library(plotly) # импортируем данные # installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) installs &lt;- fread(&#39;./data/installs.csv&#39;) # payments &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv&#39;) payments &lt;- fread(&#39;./data/payments.csv&#39;) # делаем рыбу, чтобы учесть потерянные дни, в которых не было платежей arpu_fish &lt;- data.table( dt = seq(as.Date(&#39;2022-06-01&#39;), as.Date(&#39;2022-07-31&#39;), by = 1) ) arpu_fish &lt;- arpu_fish[, list(lifetime = 0:30), by = dt] # корректируем медиасорсы installs[media_source %in% c(&#39;organic&#39;, &#39;other&#39;) | is.na(media_source), media_source := &#39;organic&#39;] level 2 (HNTR) Постройте график накопительной конверсии в когорте июньских пользователей с разбивкой по источнику пользователей. # к платежам присоединяем дату инсталла и источник трафика conversion &lt;- merge( payments, installs[, list(user_pseudo_id, media_source, dt)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # берем июньскую когорту и ограничиваем даты лайфтайма conversion[, lifetime := pay_dt - dt] conversion &lt;- conversion[dt &lt; &#39;2022-07-01&#39;] conversion &lt;- conversion[lifetime &gt;= 0 &amp; lifetime &lt;= 30] # беру минимальный день от инсталла # это будет день первого платежа conversion_stat &lt;- conversion[, list(lifetime = min(lifetime)), by = list(user_pseudo_id, media_source)] # считаю количество пользователей, которые сделали платеж на этот день conversion_stat &lt;- conversion_stat[, list(new_payers = uniqueN(user_pseudo_id)), by = list(media_source, lifetime)] # считаю, сколько всего было пользователей в когорте # conversion_stat[, total_users := installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] conversion_stat &lt;- merge( installs[dt &lt; &#39;2022-07-01&#39;, list(total_users = uniqueN(user_pseudo_id)), by = media_source], conversion_stat, by = &#39;media_source&#39;, all.x = TRUE ) # сортирую и считаю накопительное количество пользователей, которые сделали # первый платеж в этот день от инсталла conversion_stat &lt;- conversion_stat[order(media_source, lifetime)] conversion_stat[, new_payers_cum := cumsum(new_payers), by = media_source] # считаю накопительную конверсию в платящих conversion_stat[, cum_conversion := new_payers_cum / total_users] # рисую plot_ly( conversion_stat, x = ~lifetime, y = ~cum_conversion, type = &#39;scatter&#39;, mode=&#39;lines&#39;, color = ~media_source ) %&gt;% layout( title = &#39;Накопительная конверсия в зависимости от источников трафика&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 3 (N) Посчитайте динамику метрики конверсию в платящих в дни 0, 3, 7, 30 от инсталла. На графике на оси OX должна быть дата инсталла, на оси OY – значение конверсии в день инсталла. Делать аналогично динамике ретеншена первого дня. # Посчитайте по каждой платформе динамику метрики конверсии в платящих в день инсталла. 0, 3, 7, 30 # На графике на оси OX должна быть дата инсталла, на оси OY – значение конверсии в день инсталла. # Делать аналогично динамике ретеншена первого дня. conversion_dyn &lt;- merge( payments, installs[, list(user_pseudo_id, media_source, dt)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) conversion_dyn &lt;- conversion_dyn[!is.na(dt)] # берем июньскую когорту и ограничиваем даты лайфтайма conversion_dyn[, lifetime := pay_dt - dt] # это будет день первого платежа conversion_dyn_stat &lt;- conversion_dyn[, list(lifetime = min(lifetime)), by = list(user_pseudo_id, dt)] # считаю количество пользователей, которые сделали платеж на этот день conversion_dyn_stat &lt;- conversion_dyn_stat[, list(new_payers = uniqueN(user_pseudo_id)), by = list(dt, lifetime)] # присоединяем рыбу conversion_dyn_stat &lt;- merge(arpu_fish, conversion_dyn_stat, by = c(&#39;dt&#39;, &#39;lifetime&#39;), all.x = TRUE) # считаю, сколько всего было пользователей в когорте # conversion_stat[, total_users := installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] conversion_dyn_stat &lt;- merge( installs[, list(total_users = uniqueN(user_pseudo_id)), by = dt], conversion_dyn_stat, by = &#39;dt&#39;, all.x = TRUE ) # сортирую и считаю накопительное количество пользователей, которые сделали # первый платеж в этот день от инсталла conversion_dyn_stat &lt;- conversion_dyn_stat[order(dt, lifetime)] conversion_dyn_stat[is.na(new_payers), new_payers := 0] conversion_dyn_stat[, new_payers_cum := cumsum(new_payers), by = dt] # считаю накопительную конверсию в платящих conversion_dyn_stat[, cum_conversion := new_payers_cum / total_users] # рисую plot_ly( conversion_dyn_stat[lifetime %in% c(0, 3, 7, 30)], x = ~dt, y = ~cum_conversion, type = &#39;scatter&#39;, mode=&#39;lines&#39;, color = ~as.character(lifetime) ) %&gt;% layout( title = &#39;Накопительная конверсия в дневных когортах&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Посчитайте по каждой платформе динамику метрики конверсию в платящих в день инсталла. На графике на оси OX должна быть дата инсталла, на оси OY – значение конверсии в день инсталла. Делать аналогично динамике ретеншена первого дня. conversion_dyn &lt;- merge( payments, installs[, list(user_pseudo_id, platform, dt)], by = c(&#39;user_pseudo_id&#39;, &#39;platform&#39;), all.x = TRUE ) conversion_dyn &lt;- conversion_dyn[!is.na(dt)] # берем июньскую когорту и ограничиваем даты лайфтайма conversion_dyn[, lifetime := pay_dt - dt] # это будет день первого платежа conversion_dyn_stat &lt;- conversion_dyn[, list(lifetime = min(lifetime)), by = list(user_pseudo_id, dt, platform)] # считаю количество пользователей, которые сделали платеж на этот день conversion_dyn_stat &lt;- conversion_dyn_stat[, list(new_payers = uniqueN(user_pseudo_id)), by = list(platform, dt, lifetime)] conversion_dyn_stat &lt;- merge(arpu_fish, conversion_dyn_stat, by = c(&#39;dt&#39;, &#39;lifetime&#39;), all.x = TRUE) # считаю, сколько всего было пользователей в когорте # conversion_stat[, total_users := installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] conversion_dyn_stat &lt;- merge( installs[, list(total_users = uniqueN(user_pseudo_id)), by = list(platform, dt)], conversion_dyn_stat, by = c(&#39;dt&#39;, &#39;platform&#39;), all.x = TRUE ) # сортирую и считаю накопительное количество пользователей, которые сделали # первый платеж в этот день от инсталла conversion_dyn_stat &lt;- conversion_dyn_stat[order(platform, dt, lifetime)] conversion_dyn_stat[is.na(new_payers), new_payers := 0] conversion_dyn_stat[, new_payers_cum := cumsum(new_payers), by = list(platform, dt)] # считаю накопительную конверсию в платящих conversion_dyn_stat[, cum_conversion := new_payers_cum / total_users] # рисую plot_ly( conversion_dyn_stat[lifetime == 0], x = ~dt, y = ~cum_conversion, type = &#39;scatter&#39;, mode=&#39;lines&#39;, color = ~platform ) %&gt;% layout( title = &#39;Динамика конверсии в платящих в дневных когортах в день инсталла&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 4 (UV) Постройте и нарисуйте структуру накопительного ARPU для июньских пользователей в зависимости оттого, какие offer_type покупали пользователи. Таким образом мы можем понять, какая товарная категория делает наибольший вклад в кумулятивное ARPU. ИЛИ Постройте график накопительного ARPU в когорте июньских пользователей с разбивкой по источнику пользователей. # к платежам присоединяем дату инсталла и источник трафика arpu_june &lt;- merge( payments, installs[, list(user_pseudo_id, media_source, dt)], by = &#39;user_pseudo_id&#39;, all.x = TRUE ) # берем июньскую когорту и ограничиваем даты лайфтайма arpu_june[, lifetime := pay_dt - dt] arpu_june &lt;- conversion[dt &lt; &#39;2022-07-01&#39;] arpu_june &lt;- arpu_june[lifetime &gt;= 0 &amp; lifetime &lt;= 30] # считаем деньги, которые платили в определенный день от инстала arpu_june_stat &lt;- arpu_june[, list(gross = sum(gross)), by = list(lifetime, offer_type)] # считаю, сколько всего было пользователей в когорте arpu_june_stat[, total_users := installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] # сортирую и считаю накопительную сумму денег, которую заплатили пользователи arpu_june_stat &lt;- arpu_june_stat[order(offer_type, lifetime)] arpu_june_stat[, gross_cum := cumsum(gross), by = offer_type] # считаю накопительную конверсию в платящих arpu_june_stat[, cum_ARPU := gross_cum / total_users] # рисую plot_ly( arpu_june_stat, x = ~lifetime, y = ~cum_ARPU, type = &#39;scatter&#39;, mode=&#39;none&#39;, stackgroup =&#39;one&#39;, color = ~offer_type ) %&gt;% layout( title = &#39;Накопительное ARPU по категориям&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) level 5 (N) Посчитайте по каждой платформе динамику ARPU 0, 1, 7 и 30 дней (сколько в среднем заплатили пользователи когорты в день инсталла, за 0 и 1 дни жизни в приложении, за первые 7 дней жизни, за первые 30 дней жизни в приложении). На графике на оси OX должна быть дата инсталла, на оси OY – значение ARPU, с разбивкой, по какому количеству дней от инсталла мы это считаем Делать аналогично динамике ретеншена, я показывал на занятии про ретеншен как раз близкое решение. arpu_dyn &lt;- merge( payments, installs[, list(user_pseudo_id, platform, dt)], by = c(&#39;user_pseudo_id&#39;, &#39;platform&#39;), all.x = TRUE ) # удаляю пользователей, которые пришли раньше июня arpu_dyn &lt;- conversion_dyn[!is.na(dt)] arpu_dyn[, lifetime := pay_dt - dt] arpu_dyn_stat &lt;- arpu_dyn[, list(gross = sum(gross)), by = list(dt, lifetime)] # присоединяю рыбу arpu_dyn_stat &lt;- merge(arpu_fish, arpu_dyn_stat, by = c(&#39;dt&#39;, &#39;lifetime&#39;), all.x = TRUE) # считаю, сколько всего было пользователей в когорте # conversion_stat[, total_users := installs[dt &lt; &#39;2022-07-01&#39;, uniqueN(user_pseudo_id)]] arpu_dyn_stat &lt;- merge( installs[, list(total_users = uniqueN(user_pseudo_id)), by = list(dt)], arpu_dyn_stat, by = c(&#39;dt&#39;), all.x = TRUE ) # сортирую и считаю накопительный гросс arpu_dyn_stat &lt;- arpu_dyn_stat[order(dt, lifetime)] arpu_dyn_stat[is.na(gross), gross := 0] arpu_dyn_stat[, gross_cum := cumsum(gross), by = list(dt)] # считаю накопительную конверсию в платящих arpu_dyn_stat[, cum_ARPU := gross_cum / total_users] # рисую plot_ly( arpu_dyn_stat[lifetime %in% c(0, 3, 7, 30)], x = ~dt, y = ~cum_ARPU, type = &#39;scatter&#39;, mode=&#39;lines&#39;, color = ~as.character(lifetime) ) %&gt;% layout( title = &#39;Динамика накопительного ARPU дневных когорт&#39;, yaxis = list(rangemode = &#39;tozero&#39;) ) %&gt;% config(displayModeBar = FALSE) Метрики монетизации Для июньской когорты, с разбивкой по платформе посчитать: количество пользователей количество платящих пользователей конверсия гросс арпу арппу средний размер платежа (гросс / количество платежей) среднее количество платежей на пользователя (количество платежей / количество платящих) Все в окне 30 дней от инсталла. installs_june &lt;- installs[dt &lt; &#39;2022-07-01&#39;] installs_june_stat &lt;- installs_june[, list(total_users = uniqueN(user_pseudo_id)), by = platform] payments_june &lt;- merge( installs_june[, list(user_pseudo_id, dt)], payments, by = &#39;user_pseudo_id&#39;, all = FALSE ) payments_june[, lifetime := pay_dt - dt] payments_june &lt;- payments_june[lifetime &lt;= 30] payments_june_stat &lt;- payments_june[, list( payers_30 = uniqueN(user_pseudo_id), gross_30 = sum(gross), n_transactions_30 = length(ts) ), by = platform] users_june_stat &lt;- merge( installs_june_stat, payments_june_stat, by = &#39;platform&#39;, all.x = TRUE ) users_june_stat[, gross_30 := round(gross_30)] users_june_stat[, Conversion_30 := paste0(round(payers_30 / total_users * 100, 1), &#39;%&#39;)] users_june_stat[, ARPU_30 := round(gross_30 / total_users, 3)] users_june_stat[, ARPPU_30 := round(gross_30 / payers_30, 3)] users_june_stat[, Av.Check_30 := round(gross_30 / n_transactions_30, 1)] users_june_stat[, Av.Purchases_30 := round(n_transactions_30 / payers_30, 1)] users_june_stat ## Key: &lt;platform&gt; ## platform total_users payers_30 gross_30 n_transactions_30 Conversion_30 ## &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt; &lt;int&gt; &lt;char&gt; ## 1: ANDROID 77770 1143 25452 3376 1.5% ## 2: IOS 33010 1458 65402 7346 4.4% ## ARPU_30 ARPPU_30 Av.Check_30 Av.Purchases_30 ## &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1: 0.327 22.268 7.5 3 ## 2: 1.981 44.857 8.9 5 ARPDAU Некогортная метрика – сколько в среднем приносит каждый зашедший в этот день пользователь. Обычно используется на дашбордах для мониторинга, какие сегменты пользователей как платят. ARPDAU = revenue / DAU Paying share Еще одна некогортная метрика – какая доля платящих среди зашедших в этот день. Также используется для мониторинга. Paying share = Payers / DAU Воронка платежей Доля пользователей, которые сделали второй, третий и т.д. платеж. Нужна для понимания, совершают ли пользователи повторные платежи. Дальше начинаются вопросы и интерпретации – почему не сделал второй платеж, и т. д. Алгоритм расчета: - берем таблицу платежей пользователя - сортируем по времени платежа - создаем новую колонку-счетчик (1, 2, 3) платежей для каждого пользователя - считаем, сколько пользователей сделало каждый номер платежа (т.е группируем по этому счетчику) - делим количество пользователей на сколько всего было пользователей, сделавших первый платеж (т.е. на значение из первой колонки) - рисуем барчартами опционально: лучше ограничить это все на лайфтайм (например, на 7 дней) и на количество платежей (например, 10, чтобы баров было не сильно много и график был читаемым) Как создать колонку-счетчик: # в R my_dt[, counter := 1:.N, by = uid] # в Python my_dt[&#39;counter&#39;] = my_dt.groupby(&#39;uid&#39;).cumcount() + 1 Домашнее задание level 1 (IATYTD) Внимательно разберите решения заданий (материалы конспекта). level 2 (HNTR) Рассчитайте табличку с метриками монетизации для июньской когорты. Сделайте разбивку по платформам. Попробуйте проинтерпретировать результаты. level 3 (HMP) Рассчитайте табличку с метриками монетизации для июньской и июльской когорт (должно быть две строки в табличке, отдельно на каждую когорту). Выберите правильный период лайфтайма. Попробуйте проинтерпретировать результаты. level 4 (UV) Постройте воронку платежей для июньской когорты. Сделайте разбивку по платформам. Попробуйте проинтерпретировать результаты. level 5 (N) Постройте графики ARPDAU и Paying Share. Для этого вам понадобится табличка логинов (https://gitlab.com/hse_mar/mar211f/-/raw/main/data/dau.csv). Сделайте эти метрики в разбивке по тому, как давно пользователи пришли в приложение: группа 1: 0 дней с инсталла группа 2: 1-7 дней с момента инсталла группа 3: 8-28 дней с инсталла группа 4: более 28 дней с инсталла "],["c7_monetization.html", "Метрики монетизации pt.4 Запись занятия Код занятия на Python Разбор домашнего задания", " Метрики монетизации pt.4 Запись занятия Код занятия на Python https://colab.research.google.com/drive/1V4x-LdknR1vVhgrwCIvzsQjHpN1xmOrj Разбор домашнего задания Датасеты: library(data.table) library(plotly) Sys.setlocale(&#39;LC_ALL&#39;, &#39;en_US.UTF-8&#39;) ## [1] &quot;LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=ru_RU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=ru_RU.UTF-8;LC_IDENTIFICATION=C&quot; # импортируем данные # installs &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/installs.csv&#39;) installs &lt;- fread(&#39;./data/installs.csv&#39;) # payments &lt;- fread(&#39;https://gitlab.com/hse_mar/mar211f/-/raw/main/data/payments_custom.csv&#39;) payments &lt;- fread(&#39;./data/payments.csv&#39;) # делаем рыбу, чтобы учесть потерянные дни, в которых не было платежей arpu_fish &lt;- data.table( dt = seq(as.Date(&#39;2022-06-01&#39;), as.Date(&#39;2022-07-31&#39;), by = 1) ) arpu_fish &lt;- arpu_fish[, list(lifetime = 0:30), by = dt] # корректируем медиасорсы installs[media_source %in% c(&#39;organic&#39;, &#39;other&#39;, &#39;&#39;) | is.na(media_source), media_source := &#39;organic&#39;] level 2 (HNTR) Рассчитайте табличку с метриками монетизации для июньской когорты. Сделайте разбивку по платформам. Попробуйте проинтерпретировать результаты. # выделяем инсталлы в июне installs_june &lt;- installs[dt &gt;= &#39;2022-06-01&#39; &amp; dt &lt; &#39;2022-07-01&#39;] installs_june_stat &lt;- installs_june[, list(total_users = uniqueN(user_pseudo_id)), by = list(media_source)] installs_june_stat ## media_source total_users ## &lt;char&gt; &lt;int&gt; ## 1: applovin_int 36714 ## 2: organic 43070 ## 3: unityads_int 21932 ## 4: googleadwords_int 7767 ## 5: Facebook Ads 1297 # к платежам присоединяем дату инсталла и источник трафика payments_june &lt;- merge( installs_june[, list(user_pseudo_id, media_source, dt)], payments, by = &#39;user_pseudo_id&#39;, all = FALSE ) # берем июньскую когорту и ограничиваем даты лайфтайма payments_june[, lifetime := pay_dt - dt] payments_june &lt;- payments_june[lifetime &gt;= 0 &amp; lifetime &lt;= 30] payments_june_stat &lt;- payments_june[, list( payers_30 = uniqueN(user_pseudo_id), gross_30 = sum(gross), n_transactions_30 = length(ts) ), by = list(media_source)] payments_june_stat ## media_source payers_30 gross_30 n_transactions_30 ## &lt;char&gt; &lt;int&gt; &lt;num&gt; &lt;int&gt; ## 1: organic 1000 39958.64 4444 ## 2: unityads_int 262 6061.02 974 ## 3: applovin_int 1120 38554.59 4583 ## 4: Facebook Ads 53 914.41 103 ## 5: googleadwords_int 161 4392.92 518 users_june_stat = merge( installs_june_stat, payments_june_stat, by = &#39;media_source&#39;, all.x = TRUE ) users_june_stat[, gross_30 := round(gross_30)] users_june_stat[, Conversion_30 := paste0(round(payers_30 * 100 / total_users, 1), &#39;%&#39;)] users_june_stat[, ARPU_30 := round(gross_30 / total_users, 3)] users_june_stat[, ARPPU_30 := round(gross_30 / payers_30, 3)] users_june_stat[, Av.Check_30 := round(gross_30 / n_transactions_30, 1)] users_june_stat[, Av.Purchases_30 := round(n_transactions_30 / payers_30, 1)] users_june_stat[, CPI := c(0.5, 2, .9, NA, 0.3)] users_june_stat[, RoAS := round(ARPU_30 / CPI, 3)] kableExtra::kable(users_june_stat) media_source total_users payers_30 gross_30 n_transactions_30 Conversion_30 ARPU_30 ARPPU_30 Av.Check_30 Av.Purchases_30 CPI RoAS Facebook Ads 1297 53 914 103 4.1% 0.705 17.245 8.9 1.9 0.5 1.410 applovin_int 36714 1120 38555 4583 3.1% 1.050 34.424 8.4 4.1 2.0 0.525 googleadwords_int 7767 161 4393 518 2.1% 0.566 27.286 8.5 3.2 0.9 0.629 organic 43070 1000 39959 4444 2.3% 0.928 39.959 9.0 4.4 NA NA unityads_int 21932 262 6061 974 1.2% 0.276 23.134 6.2 3.7 0.3 0.920 level 3 (HMP) Рассчитайте табличку с метриками монетизации для июньской и июльской когорт (должно быть две строки в табличке, отдельно на каждую когорту). Выберите правильный период лайфтайма. Попробуйте проинтерпретировать результаты. installs_jj &lt;- installs[dt &lt; &#39;2022-07-25&#39;] installs_jj[, month := strftime(dt, &#39;%B&#39;)] installs_jj_stat &lt;- installs_jj[, list(total_users = uniqueN(user_pseudo_id)), by = list(month)] # к платежам присоединяем дату инсталла и источник трафика payments_jj &lt;- merge( installs_jj[, list(user_pseudo_id, month, dt)], payments, by = &#39;user_pseudo_id&#39;, all = FALSE ) # берем июньскую когорту и ограничиваем даты лайфтайма payments_jj[, lifetime := pay_dt - dt] payments_jj &lt;- payments_jj[lifetime &gt;= 0 &amp; lifetime &lt;= 7] payments_jj_stat &lt;- payments_jj[, list( payers_7 = uniqueN(user_pseudo_id), gross_7 = sum(gross), n_transactions_7 = length(ts) ), by = list(month)] users_jj_stat = merge( installs_jj_stat, payments_jj_stat, by = &#39;month&#39;, all.x = TRUE ) users_jj_stat[, gross_7 := round(gross_7)] users_jj_stat[, Conversion_7 := paste0(round(payers_7 * 100 / total_users, 1), &#39;%&#39;)] users_jj_stat[, ARPU_7 := round(gross_7 / total_users, 3)] users_jj_stat[, ARPPU_7 := round(gross_7 / payers_7, 3)] users_jj_stat[, Av.Check_7 := round(gross_7 / n_transactions_7, 1)] users_jj_stat[, Av.Purchases_7 := round(n_transactions_7 / payers_7, 1)] kableExtra::kable(users_jj_stat) month total_users payers_7 gross_7 n_transactions_7 Conversion_7 ARPU_7 ARPPU_7 Av.Check_7 Av.Purchases_7 July 11843 217 4582 757 1.8% 0.387 21.115 6.1 3.5 June 110780 2059 42518 5217 1.9% 0.384 20.650 8.1 2.5 level 4 (UV) Постройте воронку платежей для июньской когорты. Сделайте разбивку по платформам. Попробуйте проинтерпретировать результаты. # выше мы уже делали вычисление дней от инсталла и фильтрацию на 30 дней payments_june &lt;- payments_june[order(user_pseudo_id, ts)] payments_june[, purchase_number := seq_len(.N), by = user_pseudo_id] # считаем, сколько пользователей сделало платеж с этим номером payments_funnel &lt;- payments_june[, list(n_users = uniqueN(user_pseudo_id)), keyby = purchase_number] # считаем посчитать долю от всего пользователей, сделавших платеж (purchase_number == 1) payments_funnel[, total_payers := n_users[purchase_number == 1]] # если у нас есть группировка, то надо отдельно считать и мерджить по ключу # рисуем payments_funnel[, share := n_users / total_payers] plot_ly(payments_funnel[purchase_number &lt;= 10], x = ~purchase_number, y = ~share, type = &#39;bar&#39;) %&gt;% layout( title = &#39;Воронка платежей&#39; ) %&gt;% config(displayModeBar = FALSE) Воронки можно считать не от первого шага, а от предыдущего. В некоторых случаях это удобнее и информативнее. # если хотим считать от предыдущего шага payments_funnel[, prev_users := shift(n_users, n = 1)] payments_funnel[, prev_share := n_users / prev_users] plot_ly(payments_funnel[purchase_number &lt;= 10], x = ~purchase_number, y = ~prev_share, type = &#39;bar&#39;) %&gt;% layout( title = &#39;Воронка платежей, доля от предыдущего&#39; ) %&gt;% config(displayModeBar = FALSE) ## Warning: Ignoring 1 observations Обе воронки сразу plot_ly(payments_funnel[purchase_number &lt;= 10], x = ~purchase_number, y = ~share, type = &#39;bar&#39;, name = &#39;% from payers&#39;) %&gt;% add_trace(y = ~prev_share, name = &#39;% from prev&#39;) %&gt;% layout( title = &#39;Воронка платежей&#39; ) %&gt;% config(displayModeBar = FALSE) ## Warning: Ignoring 1 observations "],["ux.html", "UX intro Запись занятия Полезные ссылки", " UX intro Запись занятия Полезные ссылки Customer Development и Custdev. Что это такое и в чем разница? Статья в блоге Олега Якубенкова. Телеграм-канал Юлии Кожуховой Канал о личном опыте маркетинговых и продуктовых исследований: нетривиальных случаях, труднодоступных аудиториях и работающих методах. Канал про полезное и вдохновляющее про исследования и исследователей от UX-команды Контура Регулярная подборка лучших постов про UX-исследования и смежных областей. Телеграм-сообщество UX REsearch. "],["ab-тесты.html", "A/b-тесты Запись занятия Принятие решений Проверка гипотез Rice framework Что такое А/В-тесты A/B-тесты в жизни продукта Дизайн А/В-тестов Ошибки в А/В-тестах Pro et contra Полезные материалы", " A/b-тесты Запись занятия Принятие решений Экспертная оценка Data-driven подход Data-informed подход Проверка гипотез Гипотеза - предположение о причинах наблюдаемого поведения Отличия от академических гипотез: меньше требования к точности больше опоры на согласованность с картиной мира разные инструменты (не только статистика) Аналитики регулярно сталкиваются с двумя большими классами ситуаций: резкое изменение и выход значения метрики на новое плато медленное снижение метрики во времени В таких ситуациях как раз и надо генерировать гипотезы о причинах наблюдаемой динамики. Типовые эвристики такие: ищем сегмент, на который пришлось самое большое снижение/изменение в целом смотрим, как поменялась структура аудитории (и поменялась ли) проверяем возможный список причин (технические и продуктовые изменения), для ситуаций с резким выходом на новое плато значений ищем системные изменения в поведении (для медленных изменений метрик) Rice framework Охват (Reach) Сколько пользователей затронут изменения за период? Влияние (Impact) Компания Intercom разработала пятиуровневую систему оценки для оценки воздействия проекта: 3 = массивное воздействие, 2 = сильное воздействие, 1 = среднее воздействие, 0.5 = слабое воздействие, 0.25 = минимальное воздействие. Уверенность (Confidence) Насколько вы уверены в своих оценках? : 100% = высокая уверенность 80% = средняя уверенность 50% = низкая уверенность ниже 50% — тревожный сигнал Усилия (Effort) Усилия, затрачиваемые всей командой на проект на всех стадиях. Что такое А/В-тесты Определение Аб-тесты пришли из экспериментальных наук, в частности, из медицины, психологии, даже сельского хозяйства. Основная идея - сравниваем разные варианты - разные методы лечения, разные условия развития, в общем виде - разные методы воздействия. А/В-тесты (сплит-тесты) - способ понять, станет ли продукт лучше, изменив часть продукта и сравнив с неизмененной частью А - контрольная группа, группа без изменений В - тестовая группа Области применения E-commerce / маркетинг - тестирование лендингов, иконок в сторе, рекламных креативов UI - дизайн пользовательских интерфейсов, например, цвет или форма кнопок Продукт - новые фичи, будь то предложения, схемы монетизации, новый контент или же просто функционал Оффлайновые тесты - например, расстановка продуктов в магазинах A/B-тесты в жизни продукта Cтартапам тесты не нужны Тесты - достаточно сложная парадигма, а стартапы в первую очередь ориентированы на реализацию идеи продукта, ключевые метрики. К тому же слишком много изменений - и на новых продуктах, и в каждом релизе у нас с десяток фичей, непонятно, как это тестировать. Или очень дорого. Да и в прод проще выкатить, так как на малом количестве пользователей можно позволить себе больше экспериментов При этом можно тестировать самые базовые вещи на этапе концепта, в играх это сеттинг и визуальный стиль. Буквально парой фейковых страниц в сторе. А/В-тесты нужны для улучшения продукта Аналитика больше для зрелых компаний, которые задумываются о сокращении расходов, а также приходят к стадии улучшения качества фичей и дополнительного функционала. Полишинг и экономия. А/В-тесты и другие инструменты Дизайн А/В-тестов Общая структура сформулировать и приоритезировать гипотезы выбрать метрику определить размер выборки разбить на группы показать пользователям проанализировать рассказать о результатах Cоставление и выбор гипотезы Какие гипотезы стоит проверять Высокая ценность и высокий риск. Тестируйте, тщательно исследуйте все детали. Высокая ценность, уверенность и низкие риски. Запускайте в первую очередь. Низкая ценность и низкие риски. Не тестируйте и, возможно, не запускайте вовсе. Низкая ценность и высокие риски. Не стоит ваших усилий, в помойку. выбор метрики Почему мы уверены, что эта фича окажет влияние? Нередко хотят тестировать какие-то минорные изменения, например, чуть скругленная иконка или чуть более яркие плашки в банке. Для компаний типа Гугла это нормально, но для нас бессмысленно - у нас есть более сильные факторы и гипотезы. Общая идея - тестировать надо то, что хотя бы в теории может сильно повлиять на продукт. И проведение теста сильно меньше возможной прибыли. Новое предложение, новая схема монетизации и так далее. Если фича не может повлиять на продукт - то зачем она нужна? Также, если фича даже в теории не окажет негативного эффекта, то можно и так пушить. Впрочем, для компаний, которые борются за доли процента в метриках, это может быть иначе - там затраты меньше, чем возможная прибыль. Как мы поймем, что фича оказала влияние? Очень большая проблема - нередко фичи делаются просто потому что делаются. Так было запланировано, так захотел продакт, начальник или кто-то еще. Важно понимать, как мы будем измерять эффект. Потому что если нет измерения - то смысл делать аб-тест? К тому же, понимание, на что влияет тестируемый фактор - это всегда понимание модели процесса, что должно измениться в первую очередь. Второе - понимание, какой будет эффект, нужно для расчеты выборки. Для слабых эффектов нужна большая выборка, а большая выборка может быть долго и дорого. разбиение на группы А/В, A/B/n-тесты Классический дизайн - есть контрольная группа, есть тестовая. Иногда тестовых групп может быть несколько. В редких случаях нет контрольной - когда мы выбираем что-то из альтернативы (лечение, схему монетизации и т.д.) АА/BB-тесты Специфичный вид. На мой взгляд, несколько устарелый - когда есть основания предполагать, что тестовая и контрольная группы неоднородны, их сплитят и сравнивают. ухудшающие А/В-тесты Нетривиальный дизайн, когда тестовая группа - это ухудшение продукта. Главное, создать разницу между группами. отрицательные А/В-тесты Редкий зверь. Вариация, когда выключается фича полностью (например, низкочастотная) и смотрим, как пользователи реагируют. На группы разбивать можно по-разному - случайным образом 50/50, в некоторых случаях меньшие доли - 80/20, у меня был кейс 85/10/5, потому что на систему монетизации. размер аудитории Какому количеству пользователей показывать? Всегда хочется сократить количество пользователей - их либо надо закупать (а это может быть дорого, например, 3-7 долларов), либо если они будут хуже платить, то страшно потерять деньги. Но при этом пользователей должно быть достаточно, чтобы проводить стат.анализ результатов. Все методы имеют ограничения, и в данном случае надо помнить, что чем слабее ожидаемый эффект, тем больше нужна выборка. Как долго вести тест? Это больше зависит от ситуации в продукте и ожиданиях топов. Вряд ли кто-то готов будет ждать несколько месяцев, пока наберется группа. С другой стороны, есть технические ограничения, например, нельзя за один день закупить пользователей. Плюс есть всякие факторы сезонности, праздники и проч. Это уже в ошибках тестов. размер выборки ожидаемый эффект доверительные интервалы Ошибки в А/В-тестах Ошибка парадигмы Вообще не проводить АВ-тесты Выкатить фичу на прод может быть в разы дороже, чем сделать аб-тест. Выкатить много фичей на прод одновременно - тем более, непонятно, что будет влиять. А аб-тесты структурируют план экспериментов Ошибки реализации зафиксированные тестовые и контрольная группы В настоящее время редкая, но все же встречающаяся ошибка - когда пользователь навсегда попадает в какую-то группу. Вызвано, как правило, тем, что разработчики что-то не учли, а аналитики не проконтролировали. Вредно тем, что в какой-то момент пользователи тестовой и контрольной групп начинают сильно отличаться по опыту и, соответственно, могут по-разному реагировать на тесты. технические проблемы при разделении групп Как правило, когда пользователи попадают сразу в две группы, или метка не соответствует реальности. Такое может быть, особенно когда меняется система идентификации пользователя (истекают куки сайта). Есть и целенаправленные ситуации, когда производится подтасовка - кейс RetailRocket и Rees, в котором уводились пользователи из сегмента. Ошибка в реализации тестируемых моделей Как-то мы тестировали простую схему - одним пользователям давали фиксированную скидку, другим - в зависимости от их платежной истории. Соответственно, сегменты вычислялись достаточно сложно. Важно было правильно определять сегмент и скидку для пользователя, перепроверяли работу серверников в холостых тестах. Общий вывод - аналитики должны контролировать то, как технически реализованы тесты. Ошибки дизайна игнорирование специфики метрики Надо учитывать, как ведет себя метрика, на изменение которой мы ориентируемся в тесте. Специфика метрики может накладывать свои результаты. Например, может быть недельное колебание удержания, и тогда тест надо проводить неделю, так как при коротких тестах на пару дней можно получить завышенные или заниженные результаты. Аналогично с активностью. Могут быть более длинные эффекты - например, начало учебного года или новогодние праздники. тестировать надо на той же выборке, на которой будем применять Тестировать платежку на жителях Филиппин можно, но странно. Но делают. Аналогично - тестировать на доп.офисах в спальных районах и в центрах города (или в провинции и в москве) - странная идея. должны быть исключены внешние факторы (вести группы параллельно) Последовательные группы (две рекламные кампании) или две версии - это не аб-тест. Потому что есть куча побочных факторов, которые сложно проконтролировать. тестируется сразу несколько изменений Частая ошибка, когда на одной и той же выборке тестируется два или больше вариантов. Проблема в том, что в результате получается не 2 группы, а 4, а выборка уменьшается. предложения в тестовой и контрольной группе неоднородны Иногда бывает так, что само предложение в тестовой группе влияет на целевую метрику. Например, когда мы предлагаем два пакета за одну цену, но в одном скидка чуть больше. перебор вариантов руками пользователей (рост ошибки I рода) Кейса гугла. 41 вариант цвета ссылки. Общая логика проверки гипотез - у нас есть до 5% вероятности, что группы все-таки не различаются, случайно нашли то, чего нет. Вообще, это именно ошибка дизайна - группы должны формироваться из какой-то конкретной гипотезы, а не просто искать хороший вариант. Ошибки в анализе подсматривание результатов в процессе Всем хочется побыстрее узнать результаты, поэтому смотрят постоянно на группы. Это ошибка, потому что всегда есть вариативность и даже неразличимые группы в некоторые моменты могут различаться. Притом, чем чаще подсматриваешь, тем выше вероятность сделать ложный вывод. Притом, подсматривать можно, но выводы делать только после набора выборки. Варианты решения - использование неклассических стат.парадигм, в частности, последовательного семплинга. игнорируется форма распределения Чисто техническая проблема, когда применяются некорректные методы проверки стат.гипотез игнорируется размер эффекта Важно вообще понимать, какой рычаг у теста. Ошибки в интерпретации не учитываются долгосрочные эффекты Часто вывод по аб-тестам делается в достаточно быстро после окончания теста. При этом игнорируются какие-то долгосрочные эффекты. Например, если пользователь сейчас купил товар по высокой скидке (в тестовой группе больше продаж и прибыль), надо посмотреть, а как он дальше будет покупать. В играх это проблема с банком и ростом прогрессии - в начале игры маленьких платежей достаточно для победы, на дальних этапах уже нет. При тестировании на Филиппинах можно сильно ошибиться. не учитываются косвенные эффекты (как в целом реагирует система) Может быть каннибализация - когда эффект от тестовой фичи съедает эффекты от других фич. Пользователь купил по скидке, и не стал покупать другие предложения. Тестовая группа выиграла, но в тестовой группе совокупно просели платежи. Может быть и обратная сторона - в тестовой группе платежи просесть могли по другой причине, не в тестовой фиче. слабые или отсутствующие эффекты интрепретируются как тенденции Зона “когда очень хочется” - статистически есть достаточно строгие правила принятия решений, при этом даже при отсутствующих различиях хочется интерпертировать некоторые показатели. Иногда просто показаны средние, но не показаны дисперсии (а если выборка маленькая, то штормить может сильно). отсутствие положительных различий воспринимается как неудача Все как в науке, если гипотеза не подтвердилась, то ты неудачник. Хотя аб-тесты, да и наука, в общем-то, не так работают. Отсутствие различий - тоже результат. Но да, он может быть болезнен для продакта. Иногда высказываются предположения, почему тест не сработал. Важно понимать, что это тоже гипотезы, и их тоже тестировать надо. И если бы эта гипотеза была правдоподобной, мы бы ее тестировали в первом тесте, додумались бы заранее. уверенность, что тест сломан, недостаточно долго идет и т.д. Когда тест не подтверждает гипотезу, в которую веришь - грустно и хочется продлить тест, как-нибудь отфильтровать выборку и так далее. Мой кейс с сегментированием пользователей по платежной истории - сложная система оказалась сравнима с нарисованной на коленке. неудачные эксперименты скрываются или подделываются Все как в науке - перекос в сторону открытий и новизны. В критических случаях результаты переинтрпретируются или даже подделываются. Pro et contra contra надо что-то менять Внедрение аб-тестов требует достаточно много времени и сил на разработку. Во-первых, сбор данных и разделения выборки на группы (кейс OK и Димы Бугайченко). Во-вторых, сами группы (системы ранжирования, выдачи, рекомендаций). требуют квалифицированных сотрудников АБ-тесты активно используют аппарат математической статистики, поэтому нужны люди, которые это умеют делать. В идеале, чтобы еще был опыт взаимодействия с продуктом, доменной областью. Этих людей надо искать или растить. не подтверждают гипотезы Далеко не все гипотезы подтверждаются, и это вызывает боль, спекуляции и постоянные сомнения - а вдруг что-то неправильно сделали в тесте, а гипотеза верная. pro мировой тренд на data-driven подход Общий тренд в мире в принятии решений - рациональное обоснование и попытка избежать когнитивных искажений, вызванных субъективным опытом. В этом смысле тут лидирует проверка гипотез на данных. Не чистый data-driven, но каждое решение проверяется на данных или согласовывается с данными. снижает вероятность неправильных решений и экономит деньги Разработка новой фичи может быть дорогая, эффект фичи может быть негативный и вести к убыткам. Но всегда будет сопротивление “это не фича плохая”, это сезон/праздники/трафик etc - то, что нельзя опровергнуть. Во-вторых, в тесте могут вскрыться доп.эффекты. Кейс Гарфилда из ГИ. уменьшает страх ошибки у рядовых сотрудников Когда есть система с понятной логикой принятия решения, человек, который выкатывает фичу, вынужден думать о том, что делать, если фича не работает. В идеале автоматическое тестирование и тестирование множества разных гипотез (аджайл-парадигма). автоматизация проверки многих гипотез В какой-то момент компании, которые динамично развиваются и широко работают с данными, приходят к идее систем автоматических тестов - в ОК такая система интегрирована в джиру, и тот, кто задает эксперимент, просто указывает когорты и группу метрик, а результаты сыпятся в комментарии. У нас тоже есть система аб-тестов, не настолько сильная, но и делаем мы ее недолго. Полезные материалы Очень хороший канал про A/B-тесты в телеграме дайджест всяких материалов, собирает Юрий Ветров (долгое время главный по дизайну в mail.ru) отличный материал от Олега Якубенкова, один из лучших по этой теме подглядываний в аб-тестах. Плюс есть набор ссылок. А/Б тестирование: от А до Б (неплохой набор ссылок) Ваши A/B-тесты сломаны (подробный разбор некоторых ошибок в тестах, хотя местами очень уж самовлюбленное выступление) Пара слов про выборку и размер эффекта. Немного спорное мнение про интерпретацию аб-тестов. Неплохой доклад Вита Черемисинова на конфе игроделов. Отличный плейлист по А/В-тестам от весьма известных в сообществе ребят. "],["основы-статистики.html", "Основы статистики Запись занятия Презентация", " Основы статистики Запись занятия Лектор – Сергей Матросов, лид аналитики в Пятерочке, X5 Retail Group. Презентация https://docs.google.com/presentation/d/1F2HQOWh5jgAaS6C2uc1COcRq-5Bjnr_1Ukjx3qcWQ3w/edit?usp=sharing "],["кейсы-ab-тестов.html", "Кейсы A/B-тестов Запись занятия Домашнее задание", " Кейсы A/B-тестов Запись занятия Домашнее задание level 1 (IATYTD) Задание из тестового задания на продуктового аналитика Авиасейлз. У нас есть сервис поиска дешевых авиабилетов. Основная воронка: юзер зашел на сайт, нажал “искать билеты”, открыл конкретный билет, выбрал предложение конкретного партнера и перешел к нему на сайт, купил билет на сайте. Партнер платит нам комиссию за покупку билета. К нам приходит новый потенциальный партнер и хочет, чтобы мы добавили его в поисковую выдачу. Как оценить целесообразность добавления партнера в выдачу? Можно предположить, что мы можем проводить любые эксперименты (в пределах разумного). level 2 (HNTR) Прочитайте пример расчета критерия \\(\\chi^2\\)-Пирсона. Повторите его вручную (повторите действия, которые описаны на странице). Проверьте значимость различия групп с помощью R / Python (необходимо выбрать соответствующую функцию и/или пакет). level 3 (HMP) Сгенерируйте две выборки из нормального распределения (N = 100 в каждой, средние различаются на 5). Сравните, значимо ли различие выборок с помощью R / Python с помощью \\(t\\)-критерия Стьюдента (необходимо выбрать соответствующую функцию и/или пакет). Для семплирования вам нужны функция rnorm() в R и пакет numpy в Python (там либо numpy.random.Generator.normal, либо numpy.random.normal, но это устаревшая функция). level 4 (UV) Задание из тестового задания на продуктового аналитика в Альфа-банк. Был проведен эксперимент: изменение заголовка на кнопке на главном экране подписной страницы. Сделан акцент на выгоде пользователя. Описание полей: date – Дата deviceCategory – Тип устройства sourceMedium – Источник и канал привлечения experimentVariant – Группа (варианта) эксперимента: 0 - контроль, 1 - тест clickButtonOnMain – Кликнул/не кликнул по кнопке на главной странице в рамках сеанса (1 – кликнул, или 0 – не кликнул) sessionDuration – Время проведенное на сайте в рамках сеанса Данные: AB_ab_2_1 Проверьте гипотезы: Есть ли значимое изменение в большую или меньшую сторону у клика на целевую кнопку? Изменилось ли время проведенное на сайте в рамках сеанса? Напишите, какими тестами пользовались и почему выбрали их? Напишите выводы, которые можно сделать на основе анализа. level 5 (N) Определите, какая нужна выборка для тестирования ретеншена первого дня (ret1), если учесть, что мы ожидаем ret1 = 35%, и хотим погрешность в пределах +-1%. "],["стат.html", "Стат.критерии Запись занятия Код занятия на Python Разбор домашнего задания", " Стат.критерии Запись занятия Код занятия на Python https://colab.research.google.com/drive/19wQXtrX6in8Sj305043R1hnMCF_F9tno Разбор домашнего задания level 2 (HNTR) Прочитайте пример расчета критерия \\(\\chi^2\\)-Пирсона. Повторите его вручную (повторите действия, которые описаны на странице). Проверьте значимость различия групп с помощью R / Python (необходимо выбрать соответствующую функцию и/или пакет). Для ситуаций, когда у нас есть одна категориальная группирующая переменная и бинарная зависимая переменная (вернулся/не вернулся, купил/не купил), мы используем тест пропорций (\\(\\chi^2\\)-Пирсона для таблиц 2*2 как раз сводится к нему): prop.test(c(40, 30), c(72, 78), correct = FALSE, alternative = &quot;two.sided&quot;) ## ## 2-sample test for equality of proportions without continuity correction ## ## data: c(40, 30) out of c(72, 78) ## X-squared = 4.3956, df = 1, p-value = 0.03603 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.0133635 0.3285168 ## sample estimates: ## prop 1 prop 2 ## 0.5555556 0.3846154 level 3 (HMP) Сгенерируйте две выборки из нормального распределения (N = 100 в каждой, средние различаются на 5). Сравните, значимо ли различие выборок с помощью R / Python с помощью \\(t\\)-критерия Стьюдента (необходимо выбрать соответствующую функцию и/или пакет). Для семплирования вам нужны функция rnorm() в R и пакет numpy в Python (там либо numpy.random.Generator.normal, либо numpy.random.normal, но это устаревшая функция). В случаях, когда у нас есть две группы (то есть одна из переменных – категориальная) и вторая – интервальная (деньги, время, количество пользователей и т. д.) мы используем разные тесты, в зависимости от количества наблюдений и формы распределения. Один из самых распространенных тестов – \\(t\\)-критерия Стьюдента: s1 &lt;- rnorm(100, mean = 0, sd = 1) s2 &lt;- rnorm(100, mean = 5, sd = 1) t.test(s1, s2, alternative = &quot;two.sided&quot;, var.equal = TRUE) ## ## Two Sample t-test ## ## data: s1 and s2 ## t = -39.506, df = 198, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.374199 -4.863176 ## sample estimates: ## mean of x mean of y ## -0.1214402 4.9972475 Здесь я использую метку, что дисперсии выборок одинаковые. Но это потому что я сам при генерации сэмплов указал, что sd = 1. level 4 (UV) Задание из тестового задания на продуктового аналитика в Альфа-банк. Был проведен эксперимент: изменение заголовка на кнопке на главном экране подписной страницы. Сделан акцент на выгоде пользователя. Описание полей: date – Дата deviceCategory – Тип устройства sourceMedium – Источник и канал привлечения experimentVariant – Группа (варианта) эксперимента: 0 - контроль, 1 - тест clickButtonOnMain – Кликнул/не кликнул по кнопке на главной странице в рамках сеанса (1 – кликнул, или 0 – не кликнул) sessionDuration – Время проведенное на сайте в рамках сеанса Данные: AB_ab_2_1 Проверьте гипотезы: Есть ли значимое изменение в большую или меньшую сторону у клика на целевую кнопку? Изменилось ли время проведенное на сайте в рамках сеанса? Напишите, какими тестами пользовались и почему выбрали их? Напишите выводы, которые можно сделать на основе анализа. Импортируем датасет и приводим к нормальному виду: library(data.table) dataset &lt;- fread(&#39;https://raw.githubusercontent.com/upravitelev/mar231f/refs/heads/main/data/AB_ab_2_1.csv&#39;) dataset[, sessionDuration := gsub(&#39;,&#39;, &#39;.&#39;, sessionDuration, fixed = TRUE)] dataset[, sessionDuration := as.numeric(sessionDuration)] Считаем количество нажавших на кнопку в каждой группе и количество всего пользователей: dataset_stat &lt;- dataset[, list(n_users = uniqueN(userId)), keyby = list(experimentVariant, clickButtonOnMain)] dataset_stat[, total_users := sum(n_users), keyby = experimentVariant] dataset_stat ## Key: &lt;experimentVariant&gt; ## experimentVariant clickButtonOnMain n_users total_users ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1: 0 0 1293 1485 ## 2: 0 1 192 1485 ## 3: 1 0 1325 1458 ## 4: 1 1 133 1458 Применяем тест пропорций, видим значимые различия (т.е. кнопка повлияло на количество кликов): prop.test(c(192, 133), c(1485, 1458)) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(192, 133) out of c(1485, 1458) ## X-squared = 10.471, df = 1, p-value = 0.001213 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.01481732 0.06132684 ## sample estimates: ## prop 1 prop 2 ## 0.12929293 0.09122085 Считаем значимость различий между группами по длительности сессий: t.test(sessionDuration ~ experimentVariant, data = dataset) ## ## Welch Two Sample t-test ## ## data: sessionDuration by experimentVariant ## t = -45.299, df = 2217.4, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -13.58488 -12.45749 ## sample estimates: ## mean in group 0 mean in group 1 ## 120.0380 133.0592 # альтернативна запись 1 # dataset[, t.test(sessionDuration ~ experimentVariant)] # альтернативная запись 2, python-like # t.test( # dataset[experimentVariant == 0, sessionDuration], # dataset[experimentVariant == 1, sessionDuration] # ) "],["dashboards.html", "Dashboards Запись занятия Датасеты", " Dashboards Запись занятия Датасеты installs: https://docs.google.com/spreadsheets/d/1jm9iE9hEofpn0yOOMZY7blNM3-KfbuNUZz_sY23WDmw/edit?usp=drive_link payments: https://docs.google.com/spreadsheets/d/1SNM_a-HvZQk29V2kG1-PBtI_ZsDEOLAGyG76fgOL1Fw/edit?usp=drive_link "],["sql.html", "SQL Запись занятия DBeaver Параметры подключений SQL r connectors Полезные ссылки", " SQL Запись занятия DBeaver https://dbeaver.io/download/ Параметры подключений user = “student” password = “pmsar2018” dbname = “pmsar” host = “188.225.77.95” port = 5432 SQL main structure SQL - декларативный язык с жестко закрепленным порядком операторов (зарезервированных ключевых слов) в sql-выражении. При запросе данных из таблицы обязательны select и from, остальные - по необходимости. select (указание, какие колонки таблицы должны быть в результате, аналог j в data.table- dt[i, j, by]) from (из какой таблицы или результата слияния таблиц должны быть выбраны колонки) join (какая таблица должна быть присоединена по ключу, аналог merge в R) where (набор логических выражений для фильтрация по строкам, аналогично фильтрации в data.table в разделе i - dt[i, j, by]) group by (по значениям каких колонок должна быть группировка, аналог by в data.table - dt[i, j, by]) order by (по значениям каких колонок должна быть отсортированная результирующая таблица) limit (ограничение на выдачу, сколько строк таблицы должно быть отдано) ; (завершение запроса, некоторые IDE могут за пользователя подставлять) simple query Простейшие арифметические запросы требуют select и все: select 1 + 5; Table 1: 1 records ?column? 6 select now(); Table 2: 1 records now 2024-12-26 18:21:42 select: columns selection Запросы для выбора колонок. * используется как аналог select all, то есть выбор всех колонок, которые есть в таблице. В разделе from указывается схема (набор таблиц) и таблица из этой схемы, через точку: public.chars означает таблица chars из схемы public: select * from public.chars limit 3; Table 3: 3 records row.names name height mass hair_color skin_color eye_color birth_year gender url planet_name 1 Luke Skywalker 172 77 blond fair blue 19BBY male https://swapi.co/api/people/1/ Tatooine 2 C-3PO 167 75 n/a gold yellow 112BBY n/a https://swapi.co/api/people/2/ Tatooine 3 Darth Vader 202 136 none white yellow 41.9BBY male https://swapi.co/api/people/4/ Tatooine Также в блоке select можно указать одну или несколько колонок, результат арифметических операций над колонками. select name, height, planet_name from public.chars limit 3; Table 4: 3 records name height planet_name Luke Skywalker 172 Tatooine C-3PO 167 Tatooine Darth Vader 202 Tatooine Алиасы формы column_name as new_name используются для переименования колонок или результатов вычислений. select name as char_name, height, planet_name, height * 3 as height_mult from public.chars limit 3; Table 5: 3 records char_name height planet_name height_mult Luke Skywalker 172 Tatooine 516 C-3PO 167 Tatooine 501 Darth Vader 202 Tatooine 606 where: rows selection Для фильтрации по строкам используют набор логических выражений в разделе where. where planet_name = 'Coruscant' читается как все строки, в которых в колонке planet_name встречается значение Coruscant. Строковые значения и даты указыаются в одинарных кавычках, двойные кавычки только для названий колонок и схем. select name, height, planet_name from public.chars where planet_name = &#39;Coruscant&#39;; Table 6: 3 records name height planet_name Finis Valorum 170 Coruscant Adi Gallia 184 Coruscant Jocasta Nu 167 Coruscant Логические выражения можно сочетать через операторы and и or, они аналогичны логическим операторам &amp; и | в R. select name, height, planet_name from public.chars where planet_name = &#39;Coruscant&#39; or height &gt;= 170; Table 7: Displaying records 1 - 10 name height planet_name Luke Skywalker 172 Tatooine Darth Vader 202 Tatooine Owen Lars 178 Tatooine Biggs Darklighter 183 Tatooine Anakin Skywalker 188 Tatooine Cliegg Lars 183 Tatooine Boba Fett 183 Kamino Lama Su 229 Kamino Taun We 213 Kamino Poggle the Lesser 183 Geonosis Оператор in аналогичен оператору %in% в R. Отрицание производится как not in: select name, height, planet_name from public.chars where planet_name in (&#39;Coruscant&#39;, &#39;Alderaan&#39;); Table 8: 6 records name height planet_name Leia Organa 150 Alderaan Bail Prestor Organa 191 Alderaan Raymus Antilles 188 Alderaan Finis Valorum 170 Coruscant Adi Gallia 184 Coruscant Jocasta Nu 167 Coruscant Также есть инструментарий проверки на вхождение, аналог grepl() в R. Для этого используется оператор like, а в качестве искомого выражения указывается строка, где % используются в качестве любые символы. Выражение planet_name like '%Coru%' можно прочитать как “все строки таблицы, в которых в колонке planet_name встречаются строковые значения, содержащие ‘Coru’, притом и до, и после ‘Coru’ могут быть еще символы”“. Отрицание также делается как not like, для регулярных выражений используется оператор ~: select name, height, planet_name from public.chars where planet_name like &#39;%Coru%&#39; or planet_name ~ &#39;raan&#39;; Table 9: 6 records name height planet_name Leia Organa 150 Alderaan Bail Prestor Organa 191 Alderaan Raymus Antilles 188 Alderaan Finis Valorum 170 Coruscant Adi Gallia 184 Coruscant Jocasta Nu 167 Coruscant group by: aggregations Группировки аналогичны группировкам в data.table - для каждой группы строк, выделяемых по значениям группирующей колонки, над колонками производятся вычисления (средние, суммы и проч). В примере ниже считается количество строк в таблице персонажей для каждого значения, количество уникальных персонажей (в таблице одна строка на персонажа, так что совпадает с предыдущим значением) и максимальный рост среди персонажей этой группы (всех персонажей с этой планеты): select planet_name, count(*) as n_rows, count(distinct name) as n_unique_chars, max(height) as max_height from public.chars group by planet_name limit 10; Table 10: Displaying records 1 - 10 planet_name n_rows n_unique_chars max_height Alderaan 3 3 191 Aleen Minor 1 1 79 Bespin 1 1 175 Bestine IV 1 1 180 Cato Neimoidia 1 1 191 Cerea 1 1 198 Champala 1 1 196 Chandrila 1 1 150 Concord Dawn 1 1 183 Corellia 2 2 180 join Слияние таблиц по ключу, в R аналогом выступает merge(). В зависимости от схемы присоединения, используются разные операторы джойна, чайще сего - left join (сохраняем все значения ключевой колонки в той таблице, к которой присоединяем) и inner join (сохраняем только те строки, по которым значения есть и в той таблице, к которой присоединяем, и в которой присоединяем). При этом в блоке select указываем те колонки, которые хотим получить из результата слияния. Если в таблицах используются одни и те же назания колонок, то колонки надо указывать с указанием таблицы или алиаса таблицы - table1.column_name. В разделе from указывается таблица, к которой присоединяется вторая или следующие таблицы. По возможности это должна быть самая короткая таблица. Ключ, по которому соединяются колонки - using(column_name). В том случае, когда в разных таблицах колонка-ключ называется по-разному, можно использовать выражение on table1.column_name1 = table2.column_name2. Где table1 и table2 - назания таблиц и могут быть заменены алиасами таблиц. В редких случаях в конструкции с on можно использовать знаки сравнения, чтобы фильтровать определенные значения, это может ускорять выполнение запроса, но стилистически это лучше делать в разделе where. Если через join присоединяется несколько таблиц, то они присоединяются не последовательно, к результату предыдущего джойна, а к таблице, указанной в from - то есть, порядок джойнов значения не имеет. В том случае, если используется внутренний селект (в выражении join указывается не таблица, а подзапрос, с select, from и прочими атрибутами), то для таким образом полученной таблицы нужно указать алиас. select name, height, skin_color, climate, gravity, terrain, ch.url as char_url, pl.url as planet_url from public.chars as ch left join ( select * from public.planets limit 10) as pl using(planet_name) order by name limit 5; Table 11: 5 records name height skin_color climate gravity terrain char_url planet_url Ackbar 180 brown mottle NA NA NA https://swapi.co/api/people/27/ NA Adi Gallia 184 dark temperate 1 standard cityscape, mountains https://swapi.co/api/people/55/ https://swapi.co/api/planets/9/ Anakin Skywalker 188 fair NA NA NA https://swapi.co/api/people/11/ NA Ayla Secura 178 blue NA NA NA https://swapi.co/api/people/46/ NA Bail Prestor Organa 191 tan temperate 1 standard grasslands, mountains https://swapi.co/api/people/68/ https://swapi.co/api/planets/2/ Визуальная схема вариантов джойнов (для merge() тоже полезно, для понимания аргументов all.x, all.y) r connectors Для подключения из R используется пакет RPostgreSQL. С помощью функции dbConnect(), куда аоргументами передаются параметры подключения, создается объект-коннектор. library(RPostgreSQL) con &lt;- dbConnect(PostgreSQL(max.con = 100), user = &quot;student&quot;, password = &quot;pmsar2018&quot;, dbname = &quot;pmsar&quot;, host = &quot;188.225.77.95&quot;, port = 5432) Запросы же делаются с помощью функции dbGetQuery() (простая функция для селектов, есть также отдельные функции для операций над таблицами). Первый аргумент функции - объект-коннектор, второй - строковая запись запроса. Нередко удобнее строку запроса записывать в отдельный объект: query &lt;- &quot; select name, height, planet_name from public.chars limit 3 &quot; res &lt;- dbGetQuery(conn = con, statement = query) res ## name height planet_name ## 1 Luke Skywalker 172 Tatooine ## 2 C-3PO 167 Tatooine ## 3 Darth Vader 202 Tatooine Результат выполнения выражения - объект класса data.frame. То есть, его в дальнейшем желательно переконвертировать в data.table: class(res) ## [1] &quot;data.frame&quot; После завершения работы (в идеале-после каждого запроса) соединение с базой надо закрывать: dbDisconnect(con) ## [1] TRUE Подключение и просмотр БД из RStudio В последних версиях RStudio реализовано подключение к базам данных, просмотр таблиц и вызов чистых sql-запросов - аналогично DBeaver. Для этого необходимо установить пакет odbc и соответствующие драйверы подключений баз данных (подробнее см. здесь). Само создание подключения к PosgreSQL-базе данных выглядит аналогично подключению через пакет RPostgreSQL: con_odbc &lt;- DBI::dbConnect(odbc::odbc(), # sudo apt-get install odbc-postgresql Driver = &quot;PostgreSQL Unicode&quot;, Server = &quot;188.225.77.95&quot;, Database = &quot;pmsar&quot;, UID = &quot;student&quot;, PWD = &quot;pmsar2018&quot;, Port = 5432) При этом в окне Connections (обычно в верхней правой части полей RStudio, где в Environment перечислены объекты в рабочем пространстве) появляются подключение и схемы базы данных, к которой совершено подключение. Схемы - раскрывающиеся списки, при клике выводятся все таблицы схемы. После того, как в RStudio произведено подключение таким образом, можно писать запросы в отдельных скриптах - для этого необходимо создать скрипт с расширением sql (например, File &gt; New file &gt; SQL script), а в первой строчке указать подключение, и потом нажать Preview или Ctrl+Shift+Enter: Полезные ссылки Гайды хорошего оформления sql-кода. Им необязательно следовать дословно, но все же желательно принимать во внимание. ТАк или иначе, самое главное - код должен быть лаконичным, опрятным и читабельным для коллег. style guide - я предпочитаю такой гайд, хотя он во многом может вызывать нарекания. В частности, операторы многие пишут заглавными буквами, так как это повышает их видимость в коде. Также в этом гайде критикуется разное выравнивание ключевых слов и названий таблиц (чтобы формировался “коридор”). второй гайд, один из моих коллег старается ему следовать, например. Особенно осмысленно выглядит критика префиксов в названиях колонок. список онлайн-курсов по SQL на DataCamp. Курсов, может быть, больше чем надо для реальной работы, но пройти базовые вполне можно. русскоязычный сайт-соревнование по решению задачек на SQL, некоторые задачки могут быть достаточно хардкорны. еще один неплохой ресурс с задачками по PostgreSQL. На самом деле онлайн-учебников и курсов очень много. "],["sql-advanced.html", "SQL advanced Запись занятия Части SQL data types values subquery Common tables expressions select experiments Window functions explain", " SQL advanced Запись занятия Части SQL Data Definition Language Набор команд для работы с объектами базы данных. С помощью этих команд можно создать, удалить или изменить какой-нибудь объект: таблицу, схему, функцию и т. д. Как правило для этих команд требуются дополнительные права пользователей. CREATE – для создания объектов базы данных ALTER – для изменения объектов базы данных DROP – для удаления объектов базы данных Data Manipulation Language Набор команд для работы с данными - выбор данных (из таблицы или таблиц), добавление или изменение данных, удаление данных (обычно строк из таблицы, сама таблица при этом остается на месте - для ее удаления нужно сделать DROP TABLE). SELECT – выборка данных INSERT – добавляет новые данные UPDATE – изменяет существующие данные DELETE – удаляет данные Data Control Language Организация и контроль над доступом к базе данных. Например, службе дашборда надо выдать права на чтение данных. GRANT – предоставляет пользователю или группе разрешения на определённые операции с объектом REVOKE – отзывает выданные разрешения DENY – задаёт запрет, имеющий приоритет над разрешением Transaction Control Language Команды для работы с транзакциями (группами запросов, которые выполняются пакетно, чтобы не было неконсистетности в данных). Обычно аналитики с такими задачами не сталкиваются. BEGIN – служит для определения начала транзакции COMMIT – применяет транзакцию ROLLBACK – откатывает все изменения, сделанные в контексте текущей транзакции SAVEPOINT – устанавливает промежуточную точку сохранения внутри транзакции data types list В PostgreSQL (как и в других диалектах) есть большой набор разных типов данных, от стандартных (целые числа, с дробной частью, с плавающей точкой, строки, даты) до экзотических типа ip-адресов. Подробный список типов можно посмотреть вот здесь: 8. Data Types: 8.1. Numeric Types 8.2. Monetary Types 8.3. Character Types 8.4. Binary Data Types 8.5. Date/Time Types 8.6. Boolean Type 8.7. Enumerated Types 8.8. Geometric Types 8.9. Network Address Types 8.10. Bit String Types 8.11. Text Search Types 8.12. UUID Type 8.13. XML Type 8.14. JSON Types 8.15. Arrays 8.16. Composite Types 8.17. Range Types 8.18. Domain Types 8.19. Object Identifier Types 8.20. pg_lsn Type 8.21. Pseudo-Types Numeric types При работе с числовыми типами надо помнить о такой особенности, что целые числа и числа с дробью - это разные типы. И, например, при делении целого числа на целое SQL вернет также целое число (в R будет неявное преобращование типа): select 1 / 7, 7 / 3 Table 1: 1 records ?column? ?column? 0 2 Один из самых простых вариантов явного преобразования - умножить целое число с типом numeric (то есть, на 1.000): select 1 * 1.000 / 7 Table 2: 1 records ?column? 0.1428571 Character Types Строковые типы, такие же как и в других языках программирования. select &#39;abc&#39; Table 3: 1 records ?column? abc Из полезных функций - конкатенация (слияние строк, аналог paste0() в R) и изменение регистра. select &#39;a&#39; || &#39;b&#39;, upper(&#39;abc&#39;), lower(&#39;ABC&#39;) Table 4: 1 records ?column? upper lower ab ABC abc Для работы со строковыми данными есть большая группа функций, использующих регулярные выражения. Вообще, регулярные выражения - весьма часто встречающаяся в жизни аналитиков вещь и их стоит освоить. Date/Time Types Даты и время. Несмотря на то, что для для людей более читабельны даты и время в ISO-представлении (‘гггг-мм-дд чч:мм:сс’), лучше использовать unix-timestamp – представление даты в виде количества секнуд с 1970-01-01. Это представление проще, удобнее для хранения, не зависит от таймзоны пользователя и базы данных. --текущая дата select current_date Table 5: 1 records date 2024-12-26 Для преобразования даты в unix-timestamp используют функцию extract() с указанием, что извлекается epoch. select extract(epoch from current_date) Table 6: 1 records date_part 1735171200 Обратное преобразование с помощью to_timestamp(): select to_timestamp(1636156800) Table 7: 1 records to_timestamp 2021-11-06 03:00:00 Даты вычитать достаточно просто, date - date. Но если надо из даты вычесть количество дней / месяцев / лет (или другой интервал), то можно воспользоваться следующей конструкцией: select current_date - interval &#39;1&#39; day Table 8: 1 records ?column? 2024-12-25 type Conversion Для преобразования типов в Postgresql обычно используют ::, также есть более классическая и распространенная во всех диалектах функция cast(): select 1 * 1.000 / 7, 1 :: numeric / 7, cast(1 as numeric) / 7 Table 9: 1 records ?column? ?column? ?column? 0.1428571 0.1428571 0.1428571 values Иногда бывают ситуации, когда надо создать таблицу в запросе - для этого можно с помощью команды values вычислить набор строк, в которых заданы значения (количество значений в строках должно быть одинаковыми). Названия колонок в создаваемой таблице можно задать с помощью as tablename(col1_name, col2_name…), по количеству создаваемых колонок. select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(var1, var2, var3) Table 10: 2 records var1 var2 var3 1 a grp1 2 b grp1 subquery Нередко в запросах надо обратиться к подвыборке из другой таблицы. Например, это может быть как в разделе join: select * -- создаем и обращаемся к первой таблице from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(var1, var2, var3) -- создаем и джойним вторую таблицу left join ( select * from ( values (&#39;2021-11-06&#39;, &#39;grp1&#39;) ) as tb2(var4, var3) ) as t2 using(var3) Table 11: 2 records var3 var1 var2 var4 grp1 1 a 2021-11-06 grp1 2 b 2021-11-06 Более простой пример с уже существующими таблицами: select * from chars left join ( select planet_name, gravity from planets where climate = &#39;temperate&#39; ) as p using(planet_name) Table 12: Displaying records 1 - 10 planet_name row.names name height mass hair_color skin_color eye_color birth_year gender url gravity Tatooine 1 Luke Skywalker 172 77 blond fair blue 19BBY male https://swapi.co/api/people/1/ NA Tatooine 2 C-3PO 167 75 n/a gold yellow 112BBY n/a https://swapi.co/api/people/2/ NA Tatooine 3 Darth Vader 202 136 none white yellow 41.9BBY male https://swapi.co/api/people/4/ NA Tatooine 4 Owen Lars 178 120 brown, grey light blue 52BBY male https://swapi.co/api/people/6/ NA Tatooine 5 Beru Whitesun lars 165 75 brown light blue 47BBY female https://swapi.co/api/people/7/ NA Tatooine 6 R5-D4 97 32 n/a white, red red unknown n/a https://swapi.co/api/people/8/ NA Tatooine 7 Biggs Darklighter 183 84 black light brown 24BBY male https://swapi.co/api/people/9/ NA Tatooine 8 Anakin Skywalker 188 84 blond fair blue 41.9BBY male https://swapi.co/api/people/11/ NA Tatooine 9 Shmi Skywalker 163 NA black fair brown 72BBY female https://swapi.co/api/people/43/ NA Tatooine 10 Cliegg Lars 183 NA brown fair blue 82BBY male https://swapi.co/api/people/62/ NA Также вложенные запросы могут быть в блоке where: select * from chars where planet_name in ( select planet_name from planets where climate = &#39;temperate&#39; ) Table 13: Displaying records 1 - 10 row.names name height mass hair_color skin_color eye_color birth_year gender url planet_name 11 Boba Fett 183 78.2 black fair brown 31.5BBY male https://swapi.co/api/people/22/ Kamino 12 Lama Su 229 88.0 none grey black unknown male https://swapi.co/api/people/72/ Kamino 13 Taun We 213 NA none grey black unknown female https://swapi.co/api/people/73/ Kamino 19 Leia Organa 150 49.0 brown light brown 19BBY female https://swapi.co/api/people/5/ Alderaan 20 Bail Prestor Organa 191 NA black tan brown 67BBY male https://swapi.co/api/people/68/ Alderaan 21 Raymus Antilles 188 79.0 brown light brown unknown male https://swapi.co/api/people/81/ Alderaan 22 Obi-Wan Kenobi 182 77.0 auburn, white fair blue-gray 57BBY male https://swapi.co/api/people/10/ Stewjon 24 Han Solo 180 80.0 brown fair brown 29BBY male https://swapi.co/api/people/14/ Corellia 25 Wedge Antilles 170 77.0 brown fair hazel 21BBY male https://swapi.co/api/people/18/ Corellia 27 Jabba Desilijic Tiure 175 NA n/a green-tan, brown orange 600BBY hermaphrodite https://swapi.co/api/people/16/ Nal Hutta Common tables expressions Общие таблицы или “выражения с with” – крайне полезный инструмент, так как позволяет создавать в запросе временные таблицы (которые живут только во время запроса и нигде не созраняются) и обращаться к этим таблицам во время запроса. Для экспериментов удобно совмещать создание таблиц из заданных значений с помощью values и операции с этими таблицами с помощью with: -- указываем, что таблицы из запросов ниже будут временными и общими для всего запроса with -- создаем первую таблицу tmp1 as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(var1, var2, var3) ), -- создаем вторую таблицу tmp2 as ( select * from ( values (&#39;2021-11-06&#39;, &#39;grp1&#39;) ) as tb2(var4, var3) ) --основная часть - пишем запрос к созданным таблицам select * from tmp1 left join tmp2 using(var3) Table 14: 2 records var3 var1 var2 var4 grp1 1 a 2021-11-06 grp1 2 b 2021-11-06 select experiments functions В блоке select можно использовать разные, временами сложные конструкции. Самое простое - какая-то операция с колонкой, например, вычисление среднего (для среднего в sql-диалектах используется функция avg()) или максимума. with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;) ) as tbl(v1, v2, v3) ) select count(*) as n_rows, count(distinct v3) as n_groups, avg(v1) as v2_avg from tmp Table 15: 1 records n_rows n_groups v2_avg 2 1 1.5 case Немного более сложный, но очень полезный инструмент - оператор логического ветвления. В R это аналог switch или вложенных ifelse. with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- открываем логическое ветвление case -- первое условие when v1 &lt; 3 then &#39;g1&#39; -- второе условие when v1 = 3 then &#39;g2&#39; -- третье условие - &quot;все прочее&quot; else &#39;g3&#39; -- закрываем ветвление и указываем, как назвать колонку end as grp2 from tmp Table 16: 5 records v1 v2 v3 grp2 1 a grp1 g1 2 b grp1 g1 3 NA grp1 g2 4 d grp2 g3 5 e grp2 g3 filter Полезная, но достаточно малоизвестная конструкция - значения в колонках можно фильтровать по значениям других колонок. with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select -- считаем количество строк, в которых в v3 есть значение grp1 count(*) filter(where v3 = &#39;grp1&#39;), -- одновременно считаем количество значений в колонке v1, для которых в v3 есть значение grp2 count(v1) filter(where v3 = &#39;grp2&#39;) from tmp Table 17: 1 records count count 3 2 Window functions row_number() over () Select-запросы в SQL предназначены в первую очередь для извлечения подвыборок (из одной или нескольких таблиц, с определенным составом колонок). Поэтому какие-то более сложные операции бывает достаточно сложно сделать. Одними из таких операций являются действия с колонками, в которых учитываются значения колонки в предыдущих строках - например, кумулятивная сумма или сумма в определенном окне (количестве строк до текущей) и тому подобные. Такие операции делаются в SQL с помощью оконных функций, где под окном понимается определенный набор строк колонки, с которыми надо выполнить какие-то операции. Один из самых простых видов оконных функций - нумерация строк: with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- row_number() - функция определения номера, over() - определение окна. -- так как в over() ничего не указано, под окном понимаются все строки таблицы row_number() over() as counter from tmp Table 18: 5 records v1 v2 v3 counter 1 a grp1 1 2 b grp1 2 3 NA grp1 3 4 d grp2 4 5 e grp2 5 over (partition by) Оконные операции можно выполнять в группах по значениям какой-то колонки, так же при этом можно сортировать строки по другим колонкам: with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- указываем, что окно бьется на группы в зависимости от значений v3 row_number() over(partition by v3) as counter, -- указываем, что окно бьется на группы в зависимости от значений v3 -- и одновременно сортируем значения по убыванию в зависимости от колонки v1 row_number() over(partition by v3 order by v1 desc) as counter_rev from tmp order by v1 Table 19: 5 records v1 v2 v3 counter counter_rev 1 a grp1 1 3 2 b grp1 2 2 3 NA grp1 3 1 4 d grp2 1 2 5 e grp2 2 1 total sum Другой пример запроса с оконной функцией – считаем общую сумму по колонке по всей таблице и записываем ее в отдельную колонку (значение суммы одно, просто размножается по количеству строк). with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- считаем сумму v1 по всем строкам таблицы sum(v1) over() as total_sum from tmp Table 20: 5 records v1 v2 v3 total_sum 1 a grp1 15 2 b grp1 15 3 NA grp1 15 4 d grp2 15 5 e grp2 15 cumulative sum Более сложная конструкция для вычисления кумулятивной суммы. Здесь мы указываем, что хотим посчитать не просто сумму, а кумулятивную сумму. Кумулятивная сумма представляется как сумма всех значений колонки от начала и до текущей строки – окно, в котором считается сумма, с каждой строкой расширяется. Такое поведение задается аргументом range, в котором указывем границы (можно и другие границы указать): with tmp as ( select * from ( values (1, &#39;a&#39;, &#39;grp1&#39;), (2, &#39;b&#39;, &#39;grp1&#39;), (3, NULL, &#39;grp1&#39;), (4, &#39;d&#39;, &#39;grp2&#39;), (5, &#39;e&#39;, &#39;grp2&#39;) ) as tbl(v1, v2, v3) ) select *, -- для каждой строки считаем сумму v1 от начала до текущей строки sum(v1) over(order by v1 range between unbounded preceding and current row) as cum_sum from tmp Table 21: 5 records v1 v2 v3 cum_sum 1 a grp1 1 2 b grp1 3 3 NA grp1 6 4 d grp2 10 5 e grp2 15 explain plan Для оптимизации можно посмотреть план запроса, который составляет оптимизатор. Умение читать и интерпретировать подобные планы приходит с опытом, чем больше - тем лучше, я не настолько хорошо знаю эту область, чтобы полноценно про нее рассказывать. Здесь просто для иллюстрации, что такое вообще есть. explain select * from chars where planet_name = &#39;Naboo&#39; Table 22: 2 records QUERY PLAN Seq Scan on chars (cost=0.00..2.96 rows=11 width=94) Filter: (planet_name = ‘Naboo’::text) analyze Когда мы явно указываем analyze, оптимизатор не просто создает план запроса, а реально выполняет запрос и выводит, сколько времени потребовалось выполнение того или иного этапа запроса. explain analyze select * from chars where planet_name = &#39;Naboo&#39; Table 23: 5 records QUERY PLAN Seq Scan on chars (cost=0.00..2.96 rows=11 width=94) (actual time=0.047..0.052 rows=11 loops=1) Filter: (planet_name = ‘Naboo’::text) Rows Removed by Filter: 66 Planning time: 0.105 ms Execution time: 0.088 ms "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
